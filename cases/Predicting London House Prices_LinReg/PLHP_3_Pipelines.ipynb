{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importeer de benodigde bibliotheken, dan gaat het later sneller \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from kaggle.api.kaggle_api_extended import KaggleApi\n",
    "import os\n",
    "import zipfile\n",
    "from pathlib import Path\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from catboost import CatBoostRegressor\n",
    "from tqdm import tqdm_notebook, tqdm\n",
    "\n",
    "import random\n",
    "\n",
    "# interactieve modus inschakelen en output verbreden\n",
    "%matplotlib inline\n",
    "pd.set_option('display.width', 800)\n",
    "\n",
    "# remove future warnings\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# We egbruiken steeds deze seed voor reproduceerbaarheid\n",
    "seed = 42\n",
    "\n",
    "# path settings\n",
    "p = Path()\n",
    "download_path = p / 'data'\n",
    "output_path = p / 'output'\n",
    "\n",
    "# Kaggle settings\n",
    "api = KaggleApi()\n",
    "api.authenticate()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overzicht files en folders\n",
    "def print_files_folders(path=p):\n",
    "    print(path)\n",
    "    for dirname, _, filenames in os.walk(download_path):\n",
    "        for filename in filenames:\n",
    "            print(os.path.join(dirname, filename))\n",
    "            if 'train.csv' in filename:\n",
    "                print(\"hier:\",os.path.join(dirname, filename))\n",
    "                train_set = os.path.join(dirname, filename)\n",
    "                break\n",
    "\n",
    "def zoek_train_set(path):\n",
    "    for dirname, _, filenames in os.walk(path):\n",
    "        for filename in filenames:\n",
    "            if 'train' in filename: \n",
    "                train_set = os.path.join(dirname, filename)\n",
    "                return train_set\n",
    "def zoek_test_set(path):\n",
    "    for dirname, _, filenames in os.walk(path):\n",
    "        for filename in filenames:\n",
    "            if 'test' in filename: \n",
    "                train_set = os.path.join(dirname, filename)\n",
    "                return train_set\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path settings\n",
    "from pathlib import Path\n",
    "p = Path()\n",
    "download_path = p / 'data'\n",
    "output_path = p / 'output'\n",
    "images_path = p / 'images'\n",
    "\n",
    "# Create the output directory if it does not exist\n",
    "if not os.path.exists(output_path):\n",
    "\t\tos.makedirs(output_path)\n",
    "# Create the images directory if it does not exist\n",
    "if not images_path.exists():\n",
    "\timages_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Functie om de output van de gemaakte plots te bewaren\n",
    "def save_fig(fig_name, tight_layout=True, fig_extension=\"png\", resolution=300):\n",
    "    path = images_path / f\"{fig_name}.{fig_extension}\"\n",
    "    if tight_layout:\n",
    "        plt.tight_layout()\n",
    "    plt.savefig(path, format=fig_extension, dpi=resolution)\n",
    "\n",
    "# Lees het bestand in\n",
    "train_set = 'data/train.csv'\n",
    "train_raw = pd.read_csv(train_set, index_col='ID') # hier gebruiken we de ID-kolom als index !!\n",
    "test = pd.read_csv('data/test.csv')\n",
    "sample_submission = pd.read_csv('data/sample_submission.csv')\n",
    "# Vanaf hier werken we verder met 'data' als de trainingset\n",
    "# train_test_split wordt dan X_train en X_valid\n",
    "data = train_raw.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data cleaning en preprocessing "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Profiling\n",
    "Om een eerste indruk te krijgen over de dataset, maken we gebruik van een profiler.   \n",
    "In het geÃ«xporteerde bestand /output/profiler.html krijg je hierna een mooi overzicht van alle data waaruit deze dataset is opgebouwd.   \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################\n",
    "# PROFILER HTML DOCUMENT #\n",
    "##########################\n",
    "\n",
    "from ydata_profiling import ProfileReport\n",
    "import os\n",
    "import webbrowser\n",
    "\n",
    "# controleer of er al een rapport is\n",
    "profiler_file = 'profiler_3_PL.html'\n",
    "profiler_path = os.path.abspath(os.path.join(output_path, profiler_file))\n",
    "if os.path.exists(profiler_path):\n",
    "\tprint(f\"profiler betsaat al. Openen...\")\n",
    "else:\n",
    "\t# Generate the profiling report, kies een goede titel\n",
    "\tprofile = ProfileReport(data, title=\"Baseline London-house-price Report\", explorative=True) # explorative=True om ook de correlaties te zien\n",
    "\n",
    "\n",
    "\n",
    "\t# Save the report as an HTML file\n",
    "\tprofile.to_file(os.path.join(output_path,profiler_file))\n",
    "\n",
    "\t# Pad naar je bestand\n",
    "\tprofiler_path = os.path.abspath(os.path.join(output_path, profiler_file))\n",
    "\tprint(f\"profiler gemaakt: {profiler_path}\")\n",
    "\n",
    "# Open het bestand in de standaardbrowser\n",
    "webbrowser.open(f\"file://{profiler_path}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In de Profiler kan je eigenlijk alles al zien.   \n",
    "Hieronder enkele functies om hier een output te krijgen    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we 'commenten' alles om bij een Run All niet steeds de outputs te moeten maken\n",
    "\"\"\"\"\n",
    "print(f\"Eerste 5 rijen\")\n",
    "print(f\"--------------\")\n",
    "print(data.head())\n",
    "print(f\"Info over de kolommen\")\n",
    "print(f\"-----------------------\")\n",
    "print(data.info())\n",
    "print(f\"statistische info over de numerieke kolommen\")\n",
    "print(\"------------------------\")\n",
    "print(f\"{data.describe(include='all')}\")         #\n",
    "print(f\"aantal rijen en kolommen)\n",
    "print(f\"------------------------\")\n",
    "print(f\"{data.shape}\")              # \n",
    "print(f\"kolomnamen\")\n",
    "print(f\"------------------------\")\n",
    "print(f\"{data.shape}\")              # \n",
    "print(f\"datatype van de kolommen\")\n",
    "print(f\"------------------------\")\n",
    "print(f\"\"{data.columns}\")            # \n",
    "print(f\"datatype van de kolommen\")\n",
    "print(f\"------------------------\")\n",
    "print(f\"{data.dtypes}\")             # \n",
    "print(f\"aantal missende waarden per kolom\")\n",
    "print(f\"------------------------\")\n",
    "print(f\"{data.isnull().sum()}\")     # \n",
    "print(f\"aantal unieke waarden per kolom\")\n",
    "print(f\"------------------------\")\n",
    "print(f\"{data.nunique()}\")          # \n",
    "\n",
    "# Nog een methode is dfsummary\n",
    "from summarytools import dfSummary\n",
    "dfSummary(data) # geeft een mooi overzicht van de data\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vooraleer te starten, eerst wat opschonen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Missende waarden invullen in test- en train-data en kilommen met te veel lege waarden droppen\n",
    "\n",
    "# Functie om NaN-waarden te behandelen\n",
    "# Lijst om kolommen met >50% missende waarden op te slaan\n",
    "cols_to_drop = []\n",
    "\n",
    "def fill_missing_values(df):\n",
    "    for col, missing_count in df.isnull().sum().items():\n",
    "        missing_ratio = missing_count / len(df)\n",
    "        \n",
    "        if 0 < missing_ratio <= 0.5:\n",
    "            if df[col].dtype == object:  # Categorische kolommen\n",
    "                df[col] = df[col].fillna(df[col].mode()[0])\n",
    "            else:  # Numerieke kolommen\n",
    "                df[col] = df[col].fillna(df[col].median())\n",
    "        \n",
    "        elif missing_ratio > 0.5: # meer dan de helft van de waarden ontbreken\n",
    "            cols_to_drop.append(col)\n",
    "\n",
    "# Kolommen met te veel missende waarden verwijderen\n",
    "data = data.drop(columns=cols_to_drop)\n",
    "\n",
    "del cols_to_drop  # Opschoonactie\n",
    "\n",
    "\n",
    "# alleen uitvoeren indien nog lege waarden\n",
    "if data.isnull().sum().sum() > 0:\n",
    "    fill_missing_values(test)\n",
    "    fill_missing_values(data)\n",
    "    print(f\"De lege waarden werden opgevuld en de kolommen met te veel missende waarden zijn verwijderd\")\n",
    "else:\n",
    "    print(\"Er zijn geen missende waarden\")\n",
    "\n",
    "# Onnodige cols schrappen\n",
    "# alleen uitvoeren indien de kolommen nog aanwezig zijn\n",
    "if 'ID' in data.columns:\n",
    "    data = data.drop(columns=['ID', 'country'])\n",
    "    print(f\"De kolommen 'ID' en 'country' zijn verwijderd\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verwijderen van enkele kolommen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# indien geen kolom postcode aanwezig is, dan deze toevoegen\n",
    "if 'postcode' not in data.columns and 'fullAddress' in data.columns:\n",
    "    # Extracteer postcode uit adres (bijvoorbeeld 'SE5 8AB' uit 'Flat 6, 7 De Crespigny Park, London, SE5 8AB')\n",
    "    data['postcode'] = data['fullAddress'].str.extract(r'(\\w{1,2}\\d{1,2} \\d{1,2}[A-Z]{1,2})', expand=False)\n",
    "# Verwijder de volledige adreskolom\n",
    "if 'fullAddress' in data.columns:\n",
    "    data.drop(columns='fullAddress', inplace=True)\n",
    "    print(f\"Full address is verwijderd en de postcode is toegevoegd.\")\n",
    "else:\n",
    "    print(\"De fullAddress kolom reeds verwijderd.\")\n",
    "\n",
    "# slechts Ã©Ã©n waarde voor country, dus ook weg ermee\n",
    "if 'country' in data.columns: \n",
    "    data.drop(columns='country', inplace=True)\n",
    "    print(f\"Country is verwijderd\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deze is belangrijk.   \n",
    "Gezien de spreiding van de 'Price' kies ik ervoor om een Log-transformatie toe te passen op deze waarden "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target-transformatie (Log-transformatie op `price`) indien niet eerder gebeurd\n",
    "if data['price'].max() > 100:\n",
    "    data['price'] = np.log1p(data['price'])  # log(1 + price) om log(0) te voorkomen\n",
    "    print(f\"Log-transformatie op 'price' uitgevoerd\")\n",
    "else:\n",
    "    print(\"Log-transformatie op 'price' werd reeds uitgevoerd\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hier gaat het gebeuren"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stap 1: Definieer de preprocessing stappen\n",
    "Label Encoding voor categorische features en StandardScaler voor numerieke features gebruiken.\n",
    "\n",
    "Stap 2: Bouw de pipeline\n",
    "We maken de pipeline voor elke modelvariant en voegen de preprocessing in de pipeline in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, OrdinalEncoder, FunctionTransformer\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.linear_model import LinearRegression, Lasso, Ridge\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV, cross_val_score\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from scipy.stats import randint\n",
    "import xgboost as xgb\n",
    "from catboost import CatBoostRegressor\n",
    "\n",
    "\n",
    "\n",
    "#  2. Splitsen in features (X) en target (y)\n",
    "X = data.drop(columns=['price'])\n",
    "y = data['price']\n",
    "\n",
    "# 3. Train-test split\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, shuffle=True, random_state=seed)\n",
    "\n",
    "# 4. Pipeline setup\n",
    "num_features = ['bathrooms', 'bedrooms', 'floorAreaSqM', 'livingRooms']  # Voorbeeld van numerieke features\n",
    "cat_features=['tenure', 'propertyType', 'currentEnergyRating']\n",
    "\n",
    "\n",
    "# Maak een ColumnTransformer voor de preprocessing van de data\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('num', StandardScaler(), num_features),\n",
    "    ('cat', OrdinalEncoder(), cat_features),\n",
    "])\n",
    "\n",
    "# Maak een pipeline voor Random Forest (of elk ander model)\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),  # Voeg de preprocessing stap toe\n",
    "    ('feature_selection', SelectFromModel(Lasso(alpha=0.01))),  # Lasso selecteert de belangrijkste features\n",
    "    ('regressor', RandomForestRegressor(n_estimators=100, \n",
    "                                        max_depth=10,\n",
    "                                        min_samples_split=5,\n",
    "                                        random_state=seed))  # Model\n",
    "])\n",
    "\n",
    "# ð¹ Hyperparameter distributies voor RandomizedSearchCV (breder bereik)\n",
    "param_dist = {\n",
    "    'regressor__n_estimators': randint(50, 300),  # Willekeurig tussen 50-300 bomen\n",
    "    'regressor__max_depth': [None, 10, 20, 30],  # Verschillende dieptes\n",
    "    'regressor__min_samples_split': randint(2, 20),  # Willekeurig 2-20 splits\n",
    "}\n",
    "\n",
    "# 1ï¸â£ Eerst snelle zoektocht met RandomizedSearchCV\n",
    "random_search = RandomizedSearchCV(pipeline, param_distributions=param_dist, \n",
    "                                   n_iter=10,  # Aantal combinaties om te testen\n",
    "                                   cv=3, scoring='r2', \n",
    "                                   n_jobs=-1, verbose=2, random_state=seed)\n",
    "\n",
    "# Train het model met RandomizedSearch\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "# Print de beste parameters uit RandomizedSearch\n",
    "print(f\"\\nBeste parameters na RandomizedSearch: {random_search.best_params_}\\n\")\n",
    "\n",
    "\n",
    "# 2ï¸â£ Fijnere zoektocht met GridSearchCV rond de beste RandomizedSearch waarden\n",
    "best_params = random_search.best_params_\n",
    "\n",
    "# GridSearch instellen op basis van de gevonden beste waardes\n",
    "param_grid = {\n",
    "    'regressor__n_estimators': [best_params['regressor__n_estimators'] - 50, \n",
    "                                best_params['regressor__n_estimators'], \n",
    "                                best_params['regressor__n_estimators'] + 50],\n",
    "    'regressor__max_depth': [best_params['regressor__max_depth']],\n",
    "    'regressor__min_samples_split': [best_params['regressor__min_samples_split'] - 2, \n",
    "                                     best_params['regressor__min_samples_split'], \n",
    "                                     best_params['regressor__min_samples_split'] + 2]\n",
    "}\n",
    "\n",
    "# GridSearch uitvoeren\n",
    "grid_search = GridSearchCV(pipeline, param_grid, cv=5, scoring='r2', \n",
    "                           n_jobs=-1, verbose=2)\n",
    "\n",
    "# Train GridSearch met de fijnafstemming\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Beste model ophalen\n",
    "best_pipeline = grid_search.best_estimator_\n",
    "print(f\"\\nBeste parameters na GridSearch: {grid_search.best_params_}\\n\")\n",
    "\n",
    "# Voorspellen met het getunede model\n",
    "y_pred = best_pipeline.predict(X_valid)\n",
    "\n",
    "# Evaluatie-metrics berekenen\n",
    "mae = mean_absolute_error(y_valid, y_pred)\n",
    "mse = mean_squared_error(y_valid, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "r2 = r2_score(y_valid, y_pred)\n",
    "\n",
    "# Print de resultaten\n",
    "print(f'MAE: {mae:.4f}')\n",
    "print(f'MSE: {mse:.4f}')\n",
    "print(f'RMSE: {rmse:.4f}')\n",
    "print(f'RÂ²: {r2:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_log = best_pipeline.predict(test)\n",
    "y_pred = np.round(np.expm1(y_pred_log),0).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nodig om het resultaat in te dienen\n",
    "sub = sample_submission.copy()\n",
    "sub['price'] = y_pred\n",
    "sub.to_csv('output/submission_cbr_log.csv', index=False)\n",
    "sub"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "homl3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
