{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notebook Logistic Regression Case\n",
    "Oefening Data Scientist \n",
    "Geert Vandezande\n",
    "\n",
    "Doel:\n",
    "- Supervised Learning toepassen\n",
    "- EDA uitvoeren op een dataset\n",
    "- Logistic Regression toepassen op de data voor classificatie\n",
    "- andere vormen van classificatie toepassen\n",
    "\n",
    "Dataset: \n",
    "- More info: see kaggle https://www.kaggle.com/datasets/arifmia/heart-attack-risk-dataset\n",
    "\n",
    "\n",
    "Volgorde van activiteiten in deze notebook: (cfr Datacamp \"preparing data for modelling)\n",
    "- data inlezen\n",
    "- data bekijken, visueel en numerisch\n",
    "- missing data oplossen \n",
    "- incorrect types controleren\n",
    "- numerische waarde standardizeren\n",
    "- categorische varaiabelen processen\n",
    "- feature engineering\n",
    "- select features for modelling\n",
    "- eenvoudige logistic regression\n",
    "- eenvoudige logistic regression en unbalance van de features corrigeren met SMOTE- \n",
    "- modeling met Logistic Regression, Decision Tree Regression en Random Forrest Regression\n",
    "- modeling met Hyperparameter tuning met GridSearchCV voor één model\n",
    "\n",
    "\n",
    "Aantal testen uitgevoerd, met en zonder SMOTE, met en zonder HyperParameter tuning maar de resultaten veranderen niet zo veel\n",
    "Ook logistic regression met class_weight='balanced' maar opzich weinig verschil in de resultaten\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import van de diverse modules\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# preprocessing\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from collections import Counter\n",
    "from sklearn.base import TransformerMixin, BaseEstimator\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "# Machine learning algorithm\n",
    "from statsmodels.formula.api import ols\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from xgboost import plot_importance\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "# Evaluation\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, ConfusionMatrixDisplay\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, r2_score\n",
    "\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "# system utils\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "import datetime\n",
    "from colorama import Fore, Back, Style\n",
    "import sys\n",
    "import os\n",
    "import chardet\n",
    "from summarytools import dfSummary\n",
    "from tabulate import tabulate\n",
    "\n",
    "\n",
    "plot_graphs = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extra code snippits die doorheen de notebook gebruikt worden:\n",
    "\n",
    "save_fig: na generatie van een image kan de image naar file geschreven worden in de images/.. directory. Geef steeds een zinvolle naam\n",
    "\n",
    "read_JSON: om eenvoudig een JSON in te lezen\n",
    "\n",
    "log_info:\n",
    "- logging functie om doorheen de notebooks de status naar file te kunnen schrijven. \n",
    "- de logstatements worden tijdens de uitvoering van de code bewaard in een list. Die kan tussentijds naar het scherm geprint worden of naar een file\n",
    "- log_info_write_to_file: schrijf de loginformatie naar file \n",
    "- log_info_print_on_screen: print alle loginfo naar het scherm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# enkele extra code snippets gebruikt doorheen de oefening\n",
    "\n",
    "# to plot or not to plot - zet op True om de plots te zien, zet op False om de plots niet te zien bij een Run ALL\n",
    "plot_graphs = True\n",
    "\n",
    "\n",
    "# schrijf een visual naar file\n",
    "\n",
    "IMAGES_PATH = Path() / \"images\" \n",
    "IMAGES_PATH.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def save_fig(fig_id, tight_layout=True, fig_extension=\"png\", resolution=300):\n",
    "    path = IMAGES_PATH / f\"{fig_id}.{fig_extension}\"\n",
    "    if tight_layout:\n",
    "        plt.tight_layout()\n",
    "    plt.savefig(path, format=fig_extension, dpi=resolution)\n",
    "\n",
    "# Lezen van de JSON-file\n",
    "def read_JSON(file_path_read):\n",
    "    with open(file_path_read, 'r') as file:\n",
    "        files_from_json = json.load(file)\n",
    "    return files_from_json\n",
    "\n",
    "# functies om te loggen naar file\n",
    "log_info_lijst = []\n",
    "\n",
    "log_filenaam = \"LogReg_continue.log\"\n",
    "if os.path.exists(log_filenaam):\n",
    "    os.remove(log_filenaam)\n",
    "\n",
    "def log(log_code=\"INFO\", boodschap=\"euh geen boodschap????\"):\n",
    "    global log_info_lijst\n",
    "    now = datetime.datetime.now()\n",
    "    formatted_date = now.strftime(\"%d/%m/%Y %H:%M:%S\")\n",
    "    log_message = f\"{Style.RESET_ALL}{formatted_date} : {log_code} : {boodschap}\"\n",
    "    log_info_lijst.append(log_message)\n",
    "    with open(log_filenaam, 'a') as file:  # Open the file in append mode\n",
    "        file.write(boodschap + '\\n')  # Voeg een nieuwe regel toe na elke string\n",
    "    print(log_message)\n",
    "    return\n",
    "\n",
    "def log_info(boodschap):\n",
    "    log(\"Info\",boodschap)\n",
    "    def log_info_write_to_file(filename):\n",
    "        with open(filename, 'a') as file:  # Open the file in append mode\n",
    "            for string in log_info_lijst:\n",
    "                file.write(string + '\\n')  # Voeg een nieuwe regel toe na elke string\n",
    "        return\n",
    "\n",
    "def log_info_write_to_file(filename):\n",
    "    with open(filename, 'w') as file:\n",
    "        for string in log_info_lijst:\n",
    "            file.write(string + '\\n')  # Voeg een nieuwe regel toe na elke string\n",
    "    return\n",
    "\n",
    "def log_info_print_on_screen():\n",
    "    for boodschap in log_info_lijst:\n",
    "        print(boodschap)    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_histogram(dataframe, column, bins=20, color='blue', title='', xlabel='', ylabel='Frequentie', filenaam = \"Histogram\"):\n",
    "    \"\"\"\n",
    "    Deze functie plot een histogram van een gespecificeerde kolom uit een pandas DataFrame.\n",
    "    \n",
    "    Parameters:\n",
    "        dataframe (pd.DataFrame): Het DataFrame dat de data bevat.\n",
    "        column (str): De naam van de kolom waarvan een histogram moet worden geplot.\n",
    "        bins (int): Het aantal bins (groepen) voor het histogram.\n",
    "        color (str): De kleur van de histogram bars.\n",
    "        title (str): De titel van de plot.\n",
    "        xlabel (str): De label voor de x-as.\n",
    "        ylabel (str): De label voor de y-as.\n",
    "    \"\"\"\n",
    "    # Controleer of de kolom bestaat in het DataFrame\n",
    "    if column not in dataframe.columns:\n",
    "        print(f\"Kolom '{column}' niet gevonden in het DataFrame.\")\n",
    "        return\n",
    "\n",
    "    # Plot het histogram\n",
    "    plt.hist(dataframe[column], bins=bins, color=color, alpha=0.7)\n",
    "    plt.title(title if title else f'Histogram van {column}')\n",
    "    plt.xlabel(xlabel if xlabel else column)\n",
    "    plt.ylabel(ylabel)\n",
    "    if plot_graphs:\n",
    "        save_fig(filenaam)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hulpfuncties\n",
    "\n",
    "bereken_percentage_outliers: via Isolation forest wordt het percentage van de outliers berekend.\n",
    "- df : dataframe\n",
    "- columns_to_use: list van koloms die gebruikt worden om de outliers te berekenen\n",
    "- functie geeft een percentage terug van het aaantal outliers op het totaal aantal observaties\n",
    "\n",
    "\n",
    "cap_values: vervang outliers door hun lower of upperpercentieel waarde: de whiskers worden berekend door van de lower_percentieel waarde een waarde af te trekken gelijk aan 1,5 * IQR (interquartile range), voor upper_percentieel waarde wordt de 1,5 * IQR bijgeteld\n",
    "- df: dataframe\n",
    "- columns_to_use\n",
    "- lower_percentieel (default = 25)\n",
    "- upper_percentieel (default = 75)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# functie om het percentage outliers te berkenen voor een set van kolommen in een dataframe\n",
    "def bereken_percentage_aantal_outliers(df , columns_to_use):\n",
    "    # Initialiseren van het Isolation Forest model\n",
    "    iso_forest = IsolationForest(n_estimators=100, contamination='auto', random_state=42)\n",
    "\n",
    "    # Fit het model\n",
    "    iso_forest.fit(df[columns_to_use])\n",
    "    # Voorspellingen\n",
    "    # Het geeft -1 voor outliers en 1 voor inliers\n",
    "    labels = iso_forest.predict(df[columns_to_use])\n",
    "    # Toevoegen van de labels aan het DataFrame om outliers te identificeren\n",
    "    df_intern = df.copy()\n",
    "    df_intern['outlier'] = labels\n",
    "    outliers = df_intern[df_intern['outlier'] == -1]\n",
    "    aantal_outliers = df_intern['outlier'].value_counts()\n",
    "    print(aantal_outliers)\n",
    "    percentage_aantal_outliers = (len(outliers) / len(df_intern)) * 100\n",
    "\n",
    "    return percentage_aantal_outliers\n",
    "\n",
    "\n",
    "# functie om outliers in een kolom te cappen op een percentiel waarde\n",
    "def cap_values(df_input, column, lower_percentile=25, upper_percentile=75):\n",
    "    # voeg code toe om beter de outliers te verwijderen\n",
    "    log(\"Info\", f\"Capping values voor kolom {column} naar lower percentiel {lower_percentile} - upper percentiel {upper_percentile}\")\n",
    "    q1, q3 = np.percentile(df_input[column], [lower_percentile, upper_percentile])  # Calculate the 25th (Q1) and 75th (Q3) percentiles\n",
    "    iqr = q3 - q1  # Calculate the interquartile range (IQR)\n",
    "    lower_bound = q1 - 1.5 * iqr  # Calculate lower whisker (Q1 - 1.5 * IQR)\n",
    "    upper_bound = q3 + 1.5 * iqr  # Calculate upper whisker (Q3 + 1.5 * IQR)\n",
    "\n",
    "    # lower_bound = df[column].quantile(lower_percentile)\n",
    "    # upper_bound = df[column].quantile(upper_percentile)\n",
    "    \n",
    "    # Waarden cappen met behulp van de numpy.where functie\n",
    "    df_output = df_input.copy()\n",
    "    df_output[column] = np.where(df_input[column] < lower_bound, lower_bound, df_input[column])\n",
    "    df_output[column] = np.where(df_input[column] > upper_bound, upper_bound, df_input[column])    \n",
    "    return df_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data inlezen van de file\n",
    "heart_attack_risk_dataset_filename = 'data/heart_attack_risk_dataset.csv'\n",
    "df = pd.read_csv(heart_attack_risk_dataset_filename)\n",
    "log_info(f\"File ingelezen: {heart_attack_risk_dataset_filename}\")\n",
    "df_raw = df.copy() # hou het ruwe dataframe bij\n",
    "\n",
    "# lijst met features opgedeeld in numerisch en categorisch\n",
    "df_col = df.columns\n",
    "print(df_col)\n",
    "\n",
    "# bepaal eerst de opdeling van numerische en categorische waarden op basis van de kolommen\n",
    "# 3 opties\n",
    "\n",
    "# optie 1: op basis van het datatype in de kolome\n",
    "# df_num_col = df.select_dtypes(include='number').columns\n",
    "# df_cat_col = df.select_dtypes(include='object').columns\n",
    "\n",
    "\n",
    "# optie 2: op basis van de voorkomens van de waarden, indien <=4 is het een categorische \n",
    "# minder goed\n",
    "# df_cat_col = [i for i in df.columns if df[i].nunique() <= 4]\n",
    "# df_num_col= [i for i in df.columns if i not in df_cat_col]\n",
    "\n",
    "# optie 3: en dit is de manuele manier om de lijst van kolommen voor numerische en categorische waarden samen te stellen\n",
    "# check in via info() en describe() \n",
    "# we gaan dit verder gebruiken\n",
    "df_num_col = ['Age', 'BMI', 'Cholesterol_Level', 'Resting_BP', 'Heart_Rate', 'Max_Heart_Rate_Achieved']\n",
    "df_cat_col = list(set(df_col) - set(df_num_col))\n",
    "df_label_col = ['Heart_Attack_Risk'] # dit is de te voorspellen waarde\n",
    "df_cat_nom_col = ['Gender', 'Physical_Activity_Level', 'Stress_Level', 'Chest_Pain_Type', 'Thalassemia', 'ECG_Results', 'Heart_Attack_Risk']\n",
    "df_cat_ord_col = list(set(df_cat_col) - set(df_cat_nom_col))\n",
    "\n",
    "df_num = df[df_num_col]\n",
    "df_cat = df[df_cat_col]\n",
    "df_label = df[df_label_col]\n",
    "df_cat_nom = df[df_cat_nom_col]\n",
    "df_cat_ord = df[df_cat_ord_col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# eerste controles van alle waarden: \n",
    "# zijn er nulwaarden?\n",
    "# zitten er geen duplicates tussen?\n",
    "\n",
    "df.head()\n",
    "\n",
    "# Maak een samenvatting \n",
    "df_summary = df.describe().transpose()\n",
    "# Zet DataFrame om naar een mooie teksttabel\n",
    "summary_str = tabulate(df_summary, headers='keys', tablefmt='psql')\n",
    "# Print de string\n",
    "log_info(f\"\\nDescribe van de dataset\")\n",
    "log_info(summary_str)\n",
    "\n",
    "# check op nulwaarden:\n",
    "aantal_nul_waarden = df.isnull().sum()\n",
    "log_info(f\"\\nCheck op nulwaarden: \\n{aantal_nul_waarden}\")\n",
    "# ok, geen nulwaarden\n",
    "\n",
    "aantal_duplicated_waarden = df.duplicated().sum()\n",
    "log_info(f\"\\nCheck op duplicates: \\n{aantal_duplicated_waarden}\")\n",
    "# ok, geen duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We nemen eerst een snelle blik op de variabelen via een histogram plot van alle features\n",
    "# We maken een algemene plot van de variabelen\n",
    "\n",
    "if plot_graphs:\n",
    "    num_columns = 3\n",
    "    num_plots = len(df.columns)\n",
    "    num_rows = (num_plots + num_columns - 1) // num_columns\n",
    "    fig, axes = plt.subplots(num_rows, num_columns, figsize=(15, num_rows * 5))\n",
    "    axes = axes.flatten()\n",
    "    for i, column in enumerate(df.columns):\n",
    "        df[column].hist(bins=50, ax=axes[i])\n",
    "        axes[i].set_title(column)\n",
    "    # Hide any unused subplots\n",
    "    for i in range(num_plots, len(axes)):\n",
    "        fig.delaxes(axes[i])\n",
    "    plt.tight_layout()\n",
    "    save_fig(\"algemeen_overzicht_features\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# analyze van de features, eerst de numerische features\n",
    "# eerst een boxplot, voor de numerische waarde\n",
    "\n",
    "# outliers berekenen maar hier doen we voorlopig niets mee\n",
    "# dit lijkt hoog te zijn maar we gaan dit niet gebruiken\n",
    "aantal_outliers = bereken_percentage_aantal_outliers(df,['Age'])\n",
    "print(aantal_outliers)\n",
    "\n",
    "# eerste een box-plot van de numerische variabelen\n",
    "if plot_graphs:\n",
    "    fig = plt.figure(figsize=(30,10))\n",
    "    sns.boxplot(df_num)\n",
    "    save_fig(\"Numerische features boxplot van de features\")\n",
    "    plt.show()\n",
    "# Dit lijkt een normale verdeling. Er is hier geen log-verfijning nodig\n",
    "# Er zijn ook geen outliers\n",
    "\n",
    "# We maken nog een apart histogram van de numersiche variabelen\n",
    "plt.figure(figsize=(20,14))\n",
    "for i, col in enumerate(df_num_col,1):\n",
    "    plt.subplot(5,3,i)\n",
    "    sns.histplot(df[col], kde=True, palette='skyblue')\n",
    "    plt.title(f\"Distribution of {col}\")\n",
    "if plot_graphs:\n",
    "    plt.tight_layout()\n",
    "    save_fig(\"Numerische features histogram van de features\")\n",
    "    plt.show()\n",
    "# Alles lijkt ook hier normaal\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check correlatie tussen de diverse features\n",
    "\n",
    "data_num = pd.get_dummies(df, columns=['Gender','Physical_Activity_Level', 'Stress_Level','Chest_Pain_Type', 'Thalassemia', 'ECG_Results', 'Heart_Attack_Risk'], drop_first=True)\n",
    "\n",
    "plt.figure(figsize=(25,25))\n",
    "sns.heatmap(data_num.corr(), annot=True, cmap=\"coolwarm\", fmt=\".2f\")\n",
    "plt.title(\"Correlation Matrix\")\n",
    "plt.show()\n",
    "\n",
    "# weinig correlatie tussen de features\n",
    "# we kunnen de features dus behouden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# analyze van de categorische features\n",
    "print(df_cat.head())\n",
    "print(df_cat.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# maak per categorsiche variabel een bar_plot van de voorkomens van de waarden\n",
    "def bar_labels(axes, rotation= 0, location=\"edge\"):\n",
    "    for container in axes.containers:\n",
    "        axes.bar_label(container, rotation=rotation, label_type=location)\n",
    "    axes.set_ylabel(\"\")\n",
    "    axes.set_xlabel(\"\")\n",
    "    axes.set_yticklabels(())\n",
    "\n",
    "# plot de categorische waarde af met het voorkomen van de waarde in de dataset\n",
    "# 14 plots, opgedeeld in 2 rijen met 7 plots\n",
    "\n",
    "if plot_graphs:\n",
    "    index = 0\n",
    "    for r in [5,4,4]:\n",
    "        fig, axes = plt.subplots(ncols=r, figsize=(15, 6))\n",
    "        for i in range(r):\n",
    "            df[df_cat_col[index]].value_counts().plot(kind=\"bar\", ax=axes[i])\n",
    "            bar_labels(axes[i])\n",
    "            axes[i].set_title(df_cat_col[index].replace('_', ' '))            \n",
    "            index+=1      \n",
    "        plt.tight_layout()   \n",
    "        save_fig(f\"Categorische features value count van de feature deel{r}\")\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visuele voorstellen van de unbalance van de categorische variabelen\n",
    "# Creëer een figuur en een set van subplots\n",
    "num_columns = 3\n",
    "num_plots = len(df_cat.columns)\n",
    "num_rows = (num_plots + num_columns - 1) // num_columns\n",
    "fig, axes = plt.subplots(num_rows, num_columns, figsize=(15, num_rows * 5))\n",
    "\n",
    "# Flatten het axes object voor eenvoudiger iteratie\n",
    "axes = axes.flatten()\n",
    "\n",
    "# Loop over alle kolommen en creëer een pie chart voor elke kolom\n",
    "for i, column in enumerate(df_cat.columns):\n",
    "    # Haal de waarde tellingen op voor de huidige kolom\n",
    "    counts = df_cat[column].value_counts()\n",
    "    \n",
    "    # Maak een pie chart op de i-th positie in axes\n",
    "    axes[i].pie(counts, labels=counts.index, autopct='%1.1f%%', startangle=90)\n",
    "    axes[i].set_title(column)\n",
    "    axes[i].set_ylabel('')  # Verwijder de y-label voor netheid\n",
    "\n",
    "# Verberg eventuele extra subplots als het aantal kolommen minder is dan het aantal subplots\n",
    "for j in range(i + 1, len(axes)):\n",
    "    axes[j].axis('off')\n",
    "\n",
    "# Toon de plot\n",
    "plt.tight_layout()\n",
    "save_fig(f\"Categorische features unbalance view\")\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SMOTE toepassen\n",
    "# we gaan dit later mee in de pipelining integreren om te kijken wat het effect van SMOTE kan zijn\n",
    "# sample test\n",
    "\n",
    "# Voorbeeld dataset genereren\n",
    "X, y = make_classification(n_classes=2, class_sep=2, weights=[0.1, 0.9], n_informative=3, n_redundant=1, flip_y=0,\n",
    "                           n_features=20, n_clusters_per_class=1, n_samples=1000, random_state=10)\n",
    "\n",
    "# Data in een DataFrame plaatsen voor visualisatie\n",
    "df_smote = pd.DataFrame(X)\n",
    "df_smote['target'] = y\n",
    "\n",
    "# Toon de klasse distributie voor SMOTE\n",
    "print('Originele dataset shape %s' % Counter(y))\n",
    "\n",
    "# Pas SMOTE toe\n",
    "sm = SMOTE(random_state=42)\n",
    "X_res, y_res = sm.fit_resample(X, y)\n",
    "\n",
    "# Toon de nieuwe klasse distributie\n",
    "print('Resampled dataset shape %s' % Counter(y_res))\n",
    "\n",
    "# Toon de originele klasse distributie\n",
    "print('Originele dataset shape %s' % Counter(y))\n",
    "\n",
    "# Configureer de RandomUnderSampler\n",
    "rus = RandomUnderSampler(random_state=42)\n",
    "X_res_under, y_res_under = rus.fit_resample(X, y)\n",
    "\n",
    "# Toon de nieuwe klasse distributie\n",
    "print('Dataset shape na undersampling %s' % Counter(y_res_under))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# en nu gaan we op basis van de target value (\"Heart risk\") groeperen en kijken wat de impact is van de diverse\n",
    "# categorische variabelen\n",
    "\n",
    "index = 0\n",
    "grouped = df.groupby(df_label_col)\n",
    "df_group_cols = list(set(df_cat_col) - set(df_label_col))\n",
    "# plot 13 grafieken, \n",
    "if plot_graphs:\n",
    "    for j in [5, 4, 4]:\n",
    "        fig, axes = plt.subplots(ncols=j, figsize=(15, 6))\n",
    "        for i in range(j):\n",
    "            grouped[df_group_cols[index]].value_counts().unstack().plot(kind=\"bar\", stacked=True, ax=axes[i])\n",
    "            bar_labels(axes[i], 0, \"center\")\n",
    "            axes[i].set_title(df_group_cols[index].replace('_', ' '))\n",
    "            index+=1\n",
    "        plt.tight_layout()\n",
    "        save_fig(f\"Categorische features per value van de target value deel {r}\")\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hier beginnen we met de modellen\n",
    "\n",
    "Eerst een eenvoudige logistic regression, met stratefy op de target value\n",
    "Eerst zonder SMOTE, dan met parameter class_weight = 'balanced\" en dan eens met SMOTE om te zien wat het effect is van het aanpakken van de unbalanced data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eenvoudige Logistic regression met pipeline opzet \n",
    "# met stratefy op de target value\n",
    "# en zonder\n",
    "\n",
    "log_info(f\"Eenvoudige logistic regression met stratify op de target value zonder SMOTE\")\n",
    "log_info(\"**************************************************************\")\n",
    "\n",
    "df = df_raw.copy()\n",
    "\n",
    "# Separate features (X) and target variable (y)\n",
    "X = df.drop(columns=['Heart_Attack_Risk'])\n",
    "y = df['Heart_Attack_Risk']\n",
    "\n",
    "# Splitting the data into training and testing sets (80% train, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define categorical and numerical column names\n",
    "df_col = X.columns  \n",
    "df_num_col = ['Age', 'BMI', 'Cholesterol_Level', 'Resting_BP', 'Heart_Rate', 'Max_Heart_Rate_Achieved']\n",
    "df_cat_col = list(set(df_col) - set(df_num_col))\n",
    "\n",
    "num_pipeline = Pipeline([   \n",
    "    (\"standardize_numerical\", MinMaxScaler(feature_range=(-1, 1)))\n",
    "    # (\"standardize_numerical\", StandardScaler())\n",
    "])\n",
    "\n",
    "cat_pipeline = Pipeline(steps=[   \n",
    "    ('encode_categorical', OneHotEncoder(drop='first'))\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer([\n",
    "    (\"numerical\", num_pipeline, df_num_col),\n",
    "    (\"categorical\", cat_pipeline, df_cat_col)],\n",
    "     remainder='passthrough')\n",
    "\n",
    "# Create the pipeline with preprocessing and logistic regression\n",
    "pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),       \n",
    "    ('regressor', LogisticRegression())\n",
    "])\n",
    "\n",
    "# Fit the pipeline on the training data\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Making predictions on the test set\n",
    "y_pred = pipeline.predict(X_test)\n",
    "\n",
    "y_pred_counter = Counter(y_pred)\n",
    "y_test_counter = Counter(y_test)\n",
    "log_info(f\"Voorkomens in y_pred: {y_pred_counter}\")\n",
    "log_info(f\"Voorkomens in y_test: {y_test_counter}\")\n",
    "\n",
    "acc_score = accuracy_score(y_pred, y_test)*100\n",
    "class_report = classification_report(y_pred, y_test)\n",
    "conf_matrix = confusion_matrix(y_pred, y_test)\n",
    "\n",
    "log_info(f\"\\naccuracy_score: {acc_score}\")\n",
    "log_info(f\"classification_report: {class_report}\")\n",
    "summary_conf_matrix = tabulate(conf_matrix, headers='keys', tablefmt='psql')\n",
    "log_info(f\"Confusion matrix: {summary_conf_matrix}\")\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred, labels=pipeline.classes_)\n",
    "\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm,\n",
    "                              display_labels=pipeline.classes_)\n",
    "disp.plot()\n",
    "save_fig(\"Confusion matrix - example logistic regression zonder SMOTE\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eenvoudige Logistic regression met pipeline opzet \n",
    "# met stratefy op de target value\n",
    "# en nu eens met de parameter class_weight='balanced' voor de logistic regression\n",
    "\n",
    "log_info(f\"Eenvoudige logistic regression met stratify op de target value en class weight = balanced\")\n",
    "log_info(\"******************************************************************************************\")\n",
    "\n",
    "df = df_raw.copy()\n",
    "\n",
    "# Separate features (X) and target variable (y)\n",
    "X = df.drop(columns=['Heart_Attack_Risk'])\n",
    "y = df['Heart_Attack_Risk']\n",
    "\n",
    "# Splitting the data into training and testing sets (80% train, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define categorical and numerical column names\n",
    "df_col = X.columns  \n",
    "df_num_col = ['Age', 'BMI', 'Cholesterol_Level', 'Resting_BP', 'Heart_Rate', 'Max_Heart_Rate_Achieved']\n",
    "df_cat_col = list(set(df_col) - set(df_num_col))\n",
    "\n",
    "num_pipeline = Pipeline([   \n",
    "    (\"standardize_numerical\", MinMaxScaler(feature_range=(-1, 1)))\n",
    "    # (\"standardize_numerical\", StandardScaler())\n",
    "])\n",
    "\n",
    "cat_pipeline = Pipeline(steps=[   \n",
    "    ('encode_categorical', OneHotEncoder(drop='first'))\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer([\n",
    "    (\"numerical\", num_pipeline, df_num_col),\n",
    "    (\"categorical\", cat_pipeline, df_cat_col)],\n",
    "     remainder='passthrough')\n",
    "\n",
    "# Create the pipeline with preprocessing and logistic regression\n",
    "pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),       \n",
    "    ('regressor', LogisticRegression(class_weight='balanced'))\n",
    "])\n",
    "\n",
    "# Fit the pipeline on the training data\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Making predictions on the test set\n",
    "y_pred = pipeline.predict(X_test)\n",
    "\n",
    "y_pred_counter = Counter(y_pred)\n",
    "y_test_counter = Counter(y_test)\n",
    "log_info(f\"Voorkomens in y_pred: {y_pred_counter}\")\n",
    "log_info(f\"Voorkomens in y_test: {y_test_counter}\")\n",
    "\n",
    "acc_score = accuracy_score(y_pred, y_test)*100\n",
    "class_report = classification_report(y_pred, y_test)\n",
    "conf_matrix = confusion_matrix(y_pred, y_test)\n",
    "\n",
    "log_info(f\"\\naccuracy_score: {acc_score}\")\n",
    "log_info(f\"classification_report: {class_report}\")\n",
    "summary_conf_matrix = tabulate(conf_matrix, headers='keys', tablefmt='psql')\n",
    "log_info(f\"Confusion matrix: {summary_conf_matrix}\")\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred, labels=pipeline.classes_)\n",
    "\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm,\n",
    "                              display_labels=pipeline.classes_)\n",
    "disp.plot()\n",
    "save_fig(\"Confusion matrix - example logistic regression met class weight = balanced\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eenvoudige logistic regression met pipeline opzet en met SMOTE\n",
    "Om te checken of SMOTE bijdraagt tot een beter resultaat\n",
    "\n",
    "Maar dat is niet. Voor de vervolg regressie passen we geen SMOTE meer toe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eenvoudige Logistic regression met pipeline opzet en met SMOTE\n",
    "# met stratefy op de target value\n",
    "\n",
    "log_info(f\"Eenvoudige logistic regression met SMOTE en met stratify op de target value\")\n",
    "log_info(\"********************************************************************\")\n",
    "\n",
    "df = df_raw.copy()\n",
    "\n",
    "# Separate features (X) and target variable (y)\n",
    "X = df.drop(columns=['Heart_Attack_Risk'])\n",
    "y = df['Heart_Attack_Risk']\n",
    "\n",
    "# Splitting the data into training and testing sets (80% train, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define categorical and numerical column names\n",
    "df_col = X.columns  \n",
    "df_num_col = ['Age', 'BMI', 'Cholesterol_Level', 'Resting_BP', 'Heart_Rate', 'Max_Heart_Rate_Achieved']\n",
    "df_cat_col = list(set(df_col) - set(df_num_col))\n",
    "\n",
    "num_pipeline = Pipeline([   \n",
    "    (\"standardize_numerical\", MinMaxScaler(feature_range=(-1, 1)))\n",
    "    # (\"standardize_numerical\", StandardScaler())\n",
    "])\n",
    "\n",
    "cat_pipeline = Pipeline(steps=[   \n",
    "    ('encode_categorical', OneHotEncoder(drop='first'))\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer([\n",
    "    (\"numerical\", num_pipeline, df_num_col),\n",
    "    (\"categorical\", cat_pipeline, df_cat_col)],\n",
    "     remainder='passthrough')\n",
    "\n",
    "# Create the pipeline with preprocessing and logistic regression\n",
    "pipeline = ImbPipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('SMOTE', SMOTE(random_state=42)),      \n",
    "    ('regressor', LogisticRegression(class_weight='balanced'))\n",
    "])\n",
    "\n",
    "# Fit the pipeline on the training data\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Making predictions on the test set\n",
    "y_pred = pipeline.predict(X_test)\n",
    "\n",
    "y_pred_counter = Counter(y_pred)\n",
    "y_test_counter = Counter(y_test)\n",
    "log_info(f\"Voorkomens in y_pred: {y_pred_counter}\")\n",
    "log_info(f\"Voorkomens in y_test: {y_test_counter}\")\n",
    "\n",
    "acc_score = accuracy_score(y_test, y_pred)*100\n",
    "class_report = classification_report(y_test, y_pred)\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "log_info(f\"\\naccuracy_score: {acc_score}\")\n",
    "log_info(f\"classification_report: {class_report}\")\n",
    "summary_conf_matrix = tabulate(conf_matrix, headers='keys', tablefmt='psql')\n",
    "log_info(f\"Confusion matrix: {summary_conf_matrix}\")\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred, labels=pipeline.classes_)\n",
    "\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm,\n",
    "                              display_labels=pipeline.classes_)\n",
    "disp.plot()\n",
    "save_fig(\"Confusion matrix - example logistic regression met SMOTE\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nu gaan we voor een aantal classificatie modellen met pipeling\n",
    "Zonder SMOTE omdat dit niet echt bijdraagt tot het resultaat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# en nu gaan we voor een aantal classificatie modellen dit uitvoeren\n",
    "# en checken wat het beste model is tot nu toe\n",
    "# zonder SMOTE omdat we merken dat SMOTE niet bijdraagt aan een betere score\n",
    "# we gebruiken wel de parameter class_weight='balanced' om de onbalans in de target value te compenseren\n",
    "\n",
    "# duurt enkele minuten\n",
    "\n",
    "rfc = RandomForestClassifier()\n",
    "abc = AdaBoostClassifier()\n",
    "gbc = GradientBoostingClassifier()\n",
    "etc = ExtraTreesClassifier()\n",
    "svc = SVC()\n",
    "lgr = LogisticRegression(class_weight='balanced')\n",
    "\n",
    "models_cl = [rfc, abc, gbc, etc, lgr, svc]\n",
    "models_cl = [lgr, lgr, lgr]\n",
    "\n",
    "names_cl = [\"Random Forest\", \"Ada Boost\", \"Gradient Boosting\", \"Extra Trees\",\n",
    "        \"Logistic Regression\", \"Support Vector Machine\"]\n",
    "names_cl = [\"Logistic Regression\", \"Support Vector Machine\",\"logistic regression\"]\n",
    "\n",
    "def training_classification(X_train, X_test, y_train, y_test, preprocessor):    \n",
    "    scores, scores_dict, reports, cms = [], dict(), dict(), dict()\n",
    "    for i, j in zip(models_cl, names_cl):\n",
    "        pipeline = Pipeline([\n",
    "            ('preprocessor', preprocessor),                  \n",
    "            ('regressor', i)            \n",
    "            ])\n",
    "        pipeline.fit(X_train, y_train)\n",
    "        y_pred = pipeline.predict(X_test)\n",
    "        acc_score = accuracy_score(y_test, y_pred)*100\n",
    "        scores += [acc_score]\n",
    "        scores_dict[j] = [acc_score]    \n",
    "        reports[j] = classification_report(y_test, y_pred)\n",
    "        cms[j] = confusion_matrix(y_test, y_pred)\n",
    "        log_info(f\"\\n\\nModel : {j}\")\n",
    "        log_info(\"\\n\")\n",
    "        log_info(f\"Accuracy scores: {acc_score}\")\n",
    "        log_info(f\"Confusion matrix: {cms[j]}\") \n",
    "        log_info(f\"Classification report: {reports[j]}\") \n",
    "        disp = ConfusionMatrixDisplay(confusion_matrix=cm,\n",
    "                              display_labels=pipeline.classes_)\n",
    "        disp.plot()\n",
    "        fig_titel = f\"ConfusionMatrixDisplay voor model {i}\"\n",
    "        save_fig(fig_titel)\n",
    "\n",
    "    fig, axes = plt.subplots()\n",
    "    df_results = pd.DataFrame({\"score\": scores}, index=names_cl)\n",
    "    df_results = df_results.sort_values(\"score\", ascending=False)\n",
    "\n",
    "    # Een bar plot maken\n",
    "    plt.figure(figsize=(10, 5))  # Grootte van de figuur instellen\n",
    "    plt.bar(names_cl, scores, color='blue')  # Staafdiagram tekenen\n",
    "\n",
    "    # Labels en titel toevoegen\n",
    "    plt.xlabel('Model')  # Label voor de x-as\n",
    "    plt.ylabel('Accuracy score')  # Label voor de y-as\n",
    "    plt.title('Vergelijking van modellen voor classificatie')  # Titel van de plot\n",
    "\n",
    "    # De plot tonen\n",
    "    plt.show()\n",
    "\n",
    "    index = 0\n",
    "    for row in range(2):\n",
    "        fig, axes = plt.subplots(ncols=3, figsize=(15, 6))        \n",
    "        for i in range(3):\n",
    "            sns.heatmap(cms[df_results.index[index]], annot=True, fmt='d', ax=axes[i])\n",
    "            axes[i].set_title(\"{}: {}%\".format(df_results.index[index], df_results.iloc[index, 0]))\n",
    "            index += 1\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "   \n",
    "    return scores_dict, reports, cms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_raw.copy()\n",
    "\n",
    "# Separate features (X) and target variable (y)\n",
    "X = df.drop(columns=['Heart_Attack_Risk'])\n",
    "y = df['Heart_Attack_Risk']\n",
    "\n",
    "# Splitting the data into training and testing sets (80% train, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.2, random_state=42)\n",
    "\n",
    "scores_dict, reports_dict, cms_dict = training_classification(X_train, X_test, y_train, y_test, preprocessor)\n",
    "\n",
    "#results_string = tabulate(results, headers='keys', tablefmt='psql')\n",
    "for models in scores_dict.keys():\n",
    "    log_info(\"\\n\")\n",
    "    log_info(f\"Resultaten voor model {models}\")\n",
    "    log_info(\"**********************************************\\n\")\n",
    "    log_info(f\"Score: {scores_dict[models]}\")\n",
    "    log_info(f\"Classification report: {reports_dict[models]}\")\n",
    "    log_info(f\"Confusion matrix: {cms_dict[models]}\")\n",
    "    log_info(\"\\n\\n\")\n",
    "\n",
    "df = pd.DataFrame(scores_dict)\n",
    "print(df)\n",
    "df.plot(kind='bar', figsize=(10, 5))\n",
    "save_fig(\"Vergelijking van modellen voor classificatie zonder Hyperparameter tuning\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En nu gaan we de hyperparameters van de diverse modellen tunen  met GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_classification_models_with_pipeline_hyperparameter_tuning(X_train, X_test, y_train, y_test, preprocessor):\n",
    "    # Hyperparameters sets voor verschillende modellen\n",
    "\n",
    "    models = {\n",
    "        'Logistic Regression': LogisticRegression(),\n",
    "        # 'Gradient Boosting': GradientBoostingClassifier()\n",
    "    }\n",
    "\n",
    "    param_grid = {\n",
    "        'Logistic Regression': {\n",
    "            'classifier__C': [0.01, 0.1, 1.0],  # Regularisatieparameter\n",
    "            'classifier__solver': ['liblinear', 'lbfgs']  # Solvers die verschillende penalties ondersteunen\n",
    "        },\n",
    "        'Gradient Boosting': {\n",
    "            'classifier__n_estimators': [100, 200],\n",
    "            'classifier__learning_rate': [0.1, 0.05, 0.01]\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    results = {}\n",
    "    for model_name, model in models.items():\n",
    "        pipeline = Pipeline([\n",
    "            ('preprocessor', preprocessor),           \n",
    "            ('classifier', model)\n",
    "        ])     \n",
    "\n",
    "        grid_search = GridSearchCV(pipeline, param_grid[model_name], scoring=\"f1_weighted\", cv=5, refit='f1')\n",
    "        grid_search.fit(X_train, y_train)\n",
    "        best_model = grid_search.best_estimator_\n",
    "        y_pred = best_model.predict(X_test)\n",
    "\n",
    "        results[model_name] = grid_search.best_estimator_\n",
    "        best_score = grid_search.best_score_\n",
    "\n",
    "        test_score = best_model.score(X_test, y_test)\n",
    "        print(\"Score op de testset:\", test_score)\n",
    "\n",
    "        y_pred = best_model.predict(X_test)\n",
    "        conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "        class_report = classification_report(y_test, y_pred)   \n",
    "\n",
    "        print(test_score)\n",
    "\n",
    "        print(conf_matrix)\n",
    "        print(class_report)\n",
    "\n",
    "        results[model_name] = {'test_score': test_score, 'conf_matrix': conf_matrix, 'class_report': class_report}        \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define categorical and numerical column names\n",
    "df_col = X.columns  \n",
    "df_num_col = ['Age', 'BMI', 'Cholesterol_Level', 'Resting_BP', 'Heart_Rate', 'Max_Heart_Rate_Achieved']\n",
    "df_cat_col = list(set(df_col) - set(df_num_col))\n",
    "\n",
    "num_pipeline = Pipeline([   \n",
    "    (\"standardize_numerical\", MinMaxScaler(feature_range=(-1, 1)))\n",
    "    # (\"standardize_numerical\", StandardScaler())\n",
    "])\n",
    "\n",
    "cat_pipeline = Pipeline(steps=[   \n",
    "    ('encode_categorical', OneHotEncoder(drop='first'))\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer([\n",
    "    (\"numerical\", num_pipeline, df_num_col),\n",
    "    (\"categorical\", cat_pipeline, df_cat_col)],\n",
    "     remainder='passthrough')\n",
    "\n",
    "# Create the pipeline with preprocessing and logistic regression\n",
    "pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),       \n",
    "    ('regressor', LogisticRegression(class_weight='balanced'))\n",
    "])\n",
    "\n",
    "df = df_raw.copy() \n",
    "X = df.drop('Heart_Attack_Risk', axis=1)\n",
    "y = df['Heart_Attack_Risk']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2, random_state=42)\n",
    "results = execute_classification_models_with_pipeline_hyperparameter_tuning(X_train, X_test, y_train, y_test, preprocessor)\n",
    "\n",
    "df_results = pd.DataFrame(results, columns=['model', 'accuracy', 'report', 'cms'])\n",
    "resultaat_string = tabulate(df_results, headers='keys', tablefmt='psql')\n",
    "\n",
    "log_info(\"\\n\")\n",
    "log_info(f\"Resultaten voor model met hyperparameter tuning\")\n",
    "log_info(\"**********************************************\\n\")\n",
    "log_info(f\"voor models : {results.keys()}\\n\\n\")\n",
    "\n",
    "for keys, items in results.items():\n",
    "    log_info(f\"Resultaat voor model {keys}\")     \n",
    "    for k, item in items.items():\n",
    "        log_info(f\"Parameter: {k}\")     \n",
    "        log_info(f\"{item}\")    \n",
    "    log_info(\"\\n\\n\")\n",
    "\n",
    "\n",
    "# haal uit de resultsdict alle waarden van de accuracy en plot deze per model\n",
    "# en maak een confusion matrix voor elk model\n",
    "model_names = list(results.keys())\n",
    "accuracies = [results[model]['test_score'] for model in model_names]\n",
    "\n",
    "# Plot the accuracies\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.bar(model_names, accuracies, color='blue')\n",
    "plt.xlabel('Model')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Accuracy of Different Models with Hyperparameter Tuning')\n",
    "plt.xticks(rotation=45)\n",
    "save_fig(\"Model_accuracies_with_hyperparameter_tuning\")\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
