{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notebook Logistic Regression Case\n",
    "Oefening Data Scientist \n",
    "Geert Vandezande\n",
    "\n",
    "Doel:\n",
    "- Supervised Learning toepassen\n",
    "- EDA uitvoeren op een dataset\n",
    "- Logistic Regression toepassen op de data voor classificatie\n",
    "- andere vormen van classificatie toepassen\n",
    "\n",
    "Dataset: \n",
    "- More info: see kaggle https://www.kaggle.com/datasets/arifmia/heart-attack-risk-dataset\n",
    "\n",
    "\n",
    "Volgorde van activiteiten in deze notebook: (cfr Datacamp \"preparing data for modelling)\n",
    "- data inlezen\n",
    "- data bekijken, visueel en numerisch\n",
    "- missing data oplossen \n",
    "- incorrect types controleren\n",
    "- numerische waarde standardizeren\n",
    "- categorische varaiabelen processen\n",
    "- feature engineering\n",
    "- select features for modelling\n",
    "- eenvoudige logistic regression\n",
    "- eenvoudige logistic regression en unbalance van de features corrigeren met SMOTE- \n",
    "- modeling met Logistic Regression, Decision Tree Regression en Random Forrest Regression\n",
    "- modeling met Hyperparameter tuning met GridSearchCV voor één model\n",
    "\n",
    "\n",
    "Bij run ALL: \n",
    "- output in LogReg_continue.log\n",
    "- de plots worden bewaard in de images\\\n",
    "\n",
    "\n",
    "Aantal testen werdern uitgevoerd, met en zonder SMOTE, met en zonder HyperParameter tuning maar de resultaten veranderen niet zo veel\n",
    "Ook logistic regression met class_weight='balanced' maar op zich weinig verschil in de resultaten\n",
    "\n",
    "De functie zijn geschreven. We kunnen nadien nog extra testen doen met bepaalde features wel of niet mee te nemen\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import van de diverse modules\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# preprocessing\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from collections import Counter\n",
    "from sklearn.base import TransformerMixin, BaseEstimator\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "# Machine learning algorithm\n",
    "from statsmodels.formula.api import ols\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from xgboost import plot_importance\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "# Evaluation\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, ConfusionMatrixDisplay\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, r2_score\n",
    "\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "# system utils\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "import datetime\n",
    "from colorama import Fore, Back, Style\n",
    "import sys\n",
    "import os\n",
    "import chardet\n",
    "from summarytools import dfSummary\n",
    "from tabulate import tabulate\n",
    "\n",
    "\n",
    "plot_graphs = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extra code snippits die doorheen de notebook gebruikt worden:\n",
    "\n",
    "save_fig: na generatie van een image kan de image naar file geschreven worden in de images/.. directory. Geef steeds een zinvolle naam\n",
    "\n",
    "read_JSON: om eenvoudig een JSON in te lezen\n",
    "\n",
    "log_info:\n",
    "- logging functie om doorheen de notebooks de status naar file te kunnen schrijven. \n",
    "- de logstatements worden tijdens de uitvoering van de code bewaard in een list. Die kan tussentijds naar het scherm geprint worden of naar een file\n",
    "- log_info_write_to_file: schrijf de loginformatie naar file \n",
    "- log_info_print_on_screen: print alle loginfo naar het scherm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# enkele extra code snippets gebruikt doorheen de oefening\n",
    "\n",
    "# to plot or not to plot - zet op True om de plots te zien, zet op False om de plots niet te zien bij een Run ALL\n",
    "plot_graphs = True\n",
    "\n",
    "\n",
    "# schrijf een visual naar file\n",
    "\n",
    "IMAGES_PATH = Path() / \"images\" \n",
    "IMAGES_PATH.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def save_fig(fig_id, tight_layout=True, fig_extension=\"png\", resolution=300):\n",
    "    path = IMAGES_PATH / f\"{fig_id}.{fig_extension}\"\n",
    "    if tight_layout:\n",
    "        plt.tight_layout()\n",
    "    plt.savefig(path, format=fig_extension, dpi=resolution)\n",
    "\n",
    "# Lezen van de JSON-file\n",
    "def read_JSON(file_path_read):\n",
    "    with open(file_path_read, 'r') as file:\n",
    "        files_from_json = json.load(file)\n",
    "    return files_from_json\n",
    "\n",
    "# functies om te loggen naar file\n",
    "log_info_lijst = []\n",
    "\n",
    "log_filenaam = \"LogReg_continue.log\"\n",
    "if os.path.exists(log_filenaam):\n",
    "    os.remove(log_filenaam)\n",
    "\n",
    "def log(log_code=\"INFO\", boodschap=\"euh geen boodschap????\"):\n",
    "    global log_info_lijst\n",
    "    now = datetime.datetime.now()\n",
    "    formatted_date = now.strftime(\"%d/%m/%Y %H:%M:%S\")\n",
    "    log_message = f\"{Style.RESET_ALL}{formatted_date} : {log_code} : {boodschap}\"\n",
    "    log_info_lijst.append(log_message)\n",
    "    with open(log_filenaam, 'a') as file:  # Open the file in append mode\n",
    "        file.write(boodschap + '\\n')  # Voeg een nieuwe regel toe na elke string\n",
    "    print(log_message)\n",
    "    return\n",
    "\n",
    "def log_info(boodschap):\n",
    "    log(\"Info\",boodschap)\n",
    "    def log_info_write_to_file(filename):\n",
    "        with open(filename, 'a') as file:  # Open the file in append mode\n",
    "            for string in log_info_lijst:\n",
    "                file.write(string + '\\n')  # Voeg een nieuwe regel toe na elke string\n",
    "        return\n",
    "\n",
    "def log_info_write_to_file(filename):\n",
    "    with open(filename, 'w') as file:\n",
    "        for string in log_info_lijst:\n",
    "            file.write(string + '\\n')  # Voeg een nieuwe regel toe na elke string\n",
    "    return\n",
    "\n",
    "def log_info_print_on_screen():\n",
    "    for boodschap in log_info_lijst:\n",
    "        print(boodschap)    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_histogram(dataframe, column, bins=20, color='blue', title='', xlabel='', ylabel='Frequentie', filenaam = \"Histogram\"):\n",
    "    \"\"\"\n",
    "    Deze functie plot een histogram van een gespecificeerde kolom uit een pandas DataFrame.\n",
    "    \n",
    "    Parameters:\n",
    "        dataframe (pd.DataFrame): Het DataFrame dat de data bevat.\n",
    "        column (str): De naam van de kolom waarvan een histogram moet worden geplot.\n",
    "        bins (int): Het aantal bins (groepen) voor het histogram.\n",
    "        color (str): De kleur van de histogram bars.\n",
    "        title (str): De titel van de plot.\n",
    "        xlabel (str): De label voor de x-as.\n",
    "        ylabel (str): De label voor de y-as.\n",
    "    \"\"\"\n",
    "    # Controleer of de kolom bestaat in het DataFrame\n",
    "    if column not in dataframe.columns:\n",
    "        print(f\"Kolom '{column}' niet gevonden in het DataFrame.\")\n",
    "        return\n",
    "\n",
    "    # Plot het histogram\n",
    "    plt.hist(dataframe[column], bins=bins, color=color, alpha=0.7)\n",
    "    plt.title(title if title else f'Histogram van {column}')\n",
    "    plt.xlabel(xlabel if xlabel else column)\n",
    "    plt.ylabel(ylabel)\n",
    "    if plot_graphs:\n",
    "        save_fig(filenaam)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hulpfuncties\n",
    "\n",
    "bereken_percentage_outliers: via Isolation forest wordt het percentage van de outliers berekend.\n",
    "- df : dataframe\n",
    "- columns_to_use: list van koloms die gebruikt worden om de outliers te berekenen\n",
    "- functie geeft een percentage terug van het aaantal outliers op het totaal aantal observaties\n",
    "\n",
    "\n",
    "cap_values: vervang outliers door hun lower of upperpercentieel waarde: de whiskers worden berekend door van de lower_percentieel waarde een waarde af te trekken gelijk aan 1,5 * IQR (interquartile range), voor upper_percentieel waarde wordt de 1,5 * IQR bijgeteld\n",
    "- df: dataframe\n",
    "- columns_to_use\n",
    "- lower_percentieel (default = 25)\n",
    "- upper_percentieel (default = 75)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# functie om het percentage outliers te berkenen voor een set van kolommen in een dataframe\n",
    "def bereken_percentage_aantal_outliers(df , columns_to_use):\n",
    "    # Initialiseren van het Isolation Forest model\n",
    "    iso_forest = IsolationForest(n_estimators=100, contamination='auto', random_state=42)\n",
    "\n",
    "    # Fit het model\n",
    "    iso_forest.fit(df[columns_to_use])\n",
    "    # Voorspellingen\n",
    "    # Het geeft -1 voor outliers en 1 voor inliers\n",
    "    labels = iso_forest.predict(df[columns_to_use])\n",
    "    # Toevoegen van de labels aan het DataFrame om outliers te identificeren\n",
    "    df_intern = df.copy()\n",
    "    df_intern['outlier'] = labels\n",
    "    outliers = df_intern[df_intern['outlier'] == -1]\n",
    "    aantal_outliers = df_intern['outlier'].value_counts()\n",
    "    print(aantal_outliers)\n",
    "    percentage_aantal_outliers = (len(outliers) / len(df_intern)) * 100\n",
    "\n",
    "    return percentage_aantal_outliers\n",
    "\n",
    "\n",
    "# functie om outliers in een kolom te cappen op een percentiel waarde\n",
    "def cap_values(df_input, column, lower_percentile=25, upper_percentile=75):\n",
    "    # voeg code toe om beter de outliers te verwijderen\n",
    "    log(\"Info\", f\"Capping values voor kolom {column} naar lower percentiel {lower_percentile} - upper percentiel {upper_percentile}\")\n",
    "    q1, q3 = np.percentile(df_input[column], [lower_percentile, upper_percentile])  # Calculate the 25th (Q1) and 75th (Q3) percentiles\n",
    "    iqr = q3 - q1  # Calculate the interquartile range (IQR)\n",
    "    lower_bound = q1 - 1.5 * iqr  # Calculate lower whisker (Q1 - 1.5 * IQR)\n",
    "    upper_bound = q3 + 1.5 * iqr  # Calculate upper whisker (Q3 + 1.5 * IQR)\n",
    "\n",
    "    # lower_bound = df[column].quantile(lower_percentile)\n",
    "    # upper_bound = df[column].quantile(upper_percentile)\n",
    "    \n",
    "    # Waarden cappen met behulp van de numpy.where functie\n",
    "    df_output = df_input.copy()\n",
    "    df_output[column] = np.where(df_input[column] < lower_bound, lower_bound, df_input[column])\n",
    "    df_output[column] = np.where(df_input[column] > upper_bound, upper_bound, df_input[column])    \n",
    "    return df_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data inlezen van de file\n",
    "heart_attack_risk_dataset_filename = 'data/heart_attack_risk_dataset.csv'\n",
    "df = pd.read_csv(heart_attack_risk_dataset_filename)\n",
    "log_info(f\"File ingelezen: {heart_attack_risk_dataset_filename}\")\n",
    "df_raw = df.copy() # hou het ruwe dataframe bij\n",
    "\n",
    "# lijst met features opgedeeld in numerisch en categorisch\n",
    "df_col = df.columns\n",
    "print(df_col)\n",
    "\n",
    "# bepaal eerst de opdeling van numerische en categorische waarden op basis van de kolommen\n",
    "# 3 opties\n",
    "\n",
    "# optie 1: op basis van het datatype in de kolome\n",
    "# df_num_col = df.select_dtypes(include='number').columns\n",
    "# df_cat_col = df.select_dtypes(include='object').columns\n",
    "\n",
    "\n",
    "# optie 2: op basis van de voorkomens van de waarden, indien <=4 is het een categorische \n",
    "# minder goed\n",
    "# df_cat_col = [i for i in df.columns if df[i].nunique() <= 4]\n",
    "# df_num_col= [i for i in df.columns if i not in df_cat_col]\n",
    "\n",
    "# optie 3: en dit is de manuele manier om de lijst van kolommen voor numerische en categorische waarden samen te stellen\n",
    "# check in via info() en describe() \n",
    "# we gaan dit verder gebruiken\n",
    "df_num_col = ['Age', 'BMI', 'Cholesterol_Level', 'Resting_BP', 'Heart_Rate', 'Max_Heart_Rate_Achieved']\n",
    "df_cat_col = list(set(df_col) - set(df_num_col))\n",
    "df_label_col = ['Heart_Attack_Risk'] # dit is de te voorspellen waarde\n",
    "df_cat_nom_col = ['Gender', 'Physical_Activity_Level', 'Stress_Level', 'Chest_Pain_Type', 'Thalassemia', 'ECG_Results', 'Heart_Attack_Risk']\n",
    "df_cat_ord_col = list(set(df_cat_col) - set(df_cat_nom_col))\n",
    "\n",
    "df_num = df[df_num_col]\n",
    "df_cat = df[df_cat_col]\n",
    "df_label = df[df_label_col]\n",
    "df_cat_nom = df[df_cat_nom_col]\n",
    "df_cat_ord = df[df_cat_ord_col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# eerste controles van alle waarden: \n",
    "# zijn er nulwaarden?\n",
    "# zitten er geen duplicates tussen?\n",
    "\n",
    "df.head()\n",
    "\n",
    "# Maak een samenvatting \n",
    "df_summary = df.describe().transpose()\n",
    "# Zet DataFrame om naar een mooie teksttabel\n",
    "summary_str = tabulate(df_summary, headers='keys', tablefmt='psql')\n",
    "# Print de string\n",
    "log_info(f\"\\nDescribe van de dataset\")\n",
    "log_info(summary_str)\n",
    "\n",
    "# check op nulwaarden:\n",
    "aantal_nul_waarden = df.isnull().sum()\n",
    "log_info(f\"\\nCheck op nulwaarden: \\n{aantal_nul_waarden}\")\n",
    "# ok, geen nulwaarden\n",
    "\n",
    "aantal_duplicated_waarden = df.duplicated().sum()\n",
    "log_info(f\"\\nCheck op duplicates: \\n{aantal_duplicated_waarden}\")\n",
    "# ok, geen duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We nemen eerst een snelle blik op de variabelen via een histogram plot van alle features\n",
    "# We maken een algemene plot van de variabelen\n",
    "\n",
    "if plot_graphs:\n",
    "    num_columns = 3\n",
    "    num_plots = len(df.columns)\n",
    "    num_rows = (num_plots + num_columns - 1) // num_columns\n",
    "    fig, axes = plt.subplots(num_rows, num_columns, figsize=(15, num_rows * 5))\n",
    "    axes = axes.flatten()\n",
    "    for i, column in enumerate(df.columns):\n",
    "        df[column].hist(bins=50, ax=axes[i])\n",
    "        axes[i].set_title(column)\n",
    "    # Hide any unused subplots\n",
    "    for i in range(num_plots, len(axes)):\n",
    "        fig.delaxes(axes[i])\n",
    "    plt.tight_layout()\n",
    "    save_fig(\"algemeen_overzicht_features\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# analyze van de features, eerst de numerische features\n",
    "# eerst een boxplot, voor de numerische waarde\n",
    "\n",
    "# outliers berekenen maar hier doen we voorlopig niets mee\n",
    "# dit lijkt hoog te zijn maar we gaan dit niet gebruiken\n",
    "aantal_outliers = bereken_percentage_aantal_outliers(df,['Age'])\n",
    "print(aantal_outliers)\n",
    "\n",
    "# eerste een box-plot van de numerische variabelen\n",
    "if plot_graphs:\n",
    "    fig = plt.figure(figsize=(30,10))\n",
    "    sns.boxplot(df_num)\n",
    "    save_fig(\"Numerische features boxplot van de features\")\n",
    "    plt.show()\n",
    "# Dit lijkt een normale verdeling. Er is hier geen log-verfijning nodig\n",
    "# Er zijn ook geen outliers\n",
    "\n",
    "# We maken nog een apart histogram van de numersiche variabelen\n",
    "plt.figure(figsize=(20,14))\n",
    "for i, col in enumerate(df_num_col,1):\n",
    "    plt.subplot(5,3,i)\n",
    "    sns.histplot(df[col], kde=True, palette='skyblue')\n",
    "    plt.title(f\"Distribution of {col}\")\n",
    "if plot_graphs:\n",
    "    plt.tight_layout()\n",
    "    save_fig(\"Numerische features histogram van de features\")\n",
    "    plt.show()\n",
    "# Alles lijkt ook hier normaal\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check correlatie tussen de diverse features\n",
    "\n",
    "data_num = pd.get_dummies(df, columns=['Gender','Physical_Activity_Level', 'Stress_Level','Chest_Pain_Type', 'Thalassemia', 'ECG_Results', 'Heart_Attack_Risk'], drop_first=True)\n",
    "\n",
    "plt.figure(figsize=(25,25))\n",
    "sns.heatmap(data_num.corr(), annot=True, cmap=\"coolwarm\", fmt=\".2f\")\n",
    "plt.title(\"Correlation Matrix\")\n",
    "plt.show()\n",
    "\n",
    "# weinig correlatie tussen de features\n",
    "# we kunnen de features dus behouden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# analyze van de categorische features\n",
    "print(df_cat.head())\n",
    "print(df_cat.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# maak per categorsiche variabel een bar_plot van de voorkomens van de waarden\n",
    "def bar_labels(axes, rotation= 0, location=\"edge\"):\n",
    "    for container in axes.containers:\n",
    "        axes.bar_label(container, rotation=rotation, label_type=location)\n",
    "    axes.set_ylabel(\"\")\n",
    "    axes.set_xlabel(\"\")\n",
    "    axes.set_yticklabels(())\n",
    "\n",
    "# plot de categorische waarde af met het voorkomen van de waarde in de dataset\n",
    "# 14 plots, opgedeeld in 2 rijen met 7 plots\n",
    "\n",
    "if plot_graphs:\n",
    "    index = 0\n",
    "    for r in [5,4,4]:\n",
    "        fig, axes = plt.subplots(ncols=r, figsize=(15, 6))\n",
    "        for i in range(r):\n",
    "            df[df_cat_col[index]].value_counts().plot(kind=\"bar\", ax=axes[i])\n",
    "            bar_labels(axes[i])\n",
    "            axes[i].set_title(df_cat_col[index].replace('_', ' '))            \n",
    "            index+=1      \n",
    "        plt.tight_layout()   \n",
    "        save_fig(f\"Categorische features value count van de feature deel{r}\")\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visuele voorstellen van de unbalance van de categorische variabelen\n",
    "# Creëer een figuur en een set van subplots\n",
    "num_columns = 3\n",
    "num_plots = len(df_cat.columns)\n",
    "num_rows = (num_plots + num_columns - 1) // num_columns\n",
    "fig, axes = plt.subplots(num_rows, num_columns, figsize=(15, num_rows * 5))\n",
    "\n",
    "# Flatten het axes object voor eenvoudiger iteratie\n",
    "axes = axes.flatten()\n",
    "\n",
    "# Loop over alle kolommen en creëer een pie chart voor elke kolom\n",
    "for i, column in enumerate(df_cat.columns):\n",
    "    # Haal de waarde tellingen op voor de huidige kolom\n",
    "    counts = df_cat[column].value_counts()\n",
    "    \n",
    "    # Maak een pie chart op de i-th positie in axes\n",
    "    axes[i].pie(counts, labels=counts.index, autopct='%1.1f%%', startangle=90)\n",
    "    axes[i].set_title(column)\n",
    "    axes[i].set_ylabel('')  # Verwijder de y-label voor netheid\n",
    "\n",
    "# Verberg eventuele extra subplots als het aantal kolommen minder is dan het aantal subplots\n",
    "for j in range(i + 1, len(axes)):\n",
    "    axes[j].axis('off')\n",
    "\n",
    "# Toon de plot\n",
    "plt.tight_layout()\n",
    "save_fig(f\"Categorische features unbalance view\")\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SMOTE toepassen\n",
    "# we gaan dit later mee in de pipelining integreren om te kijken wat het effect van SMOTE kan zijn\n",
    "# sample test\n",
    "\n",
    "# Voorbeeld dataset genereren\n",
    "X, y = make_classification(n_classes=2, class_sep=2, weights=[0.1, 0.9], n_informative=3, n_redundant=1, flip_y=0,\n",
    "                           n_features=20, n_clusters_per_class=1, n_samples=1000, random_state=10)\n",
    "\n",
    "# Data in een DataFrame plaatsen voor visualisatie\n",
    "df_smote = pd.DataFrame(X)\n",
    "df_smote['target'] = y\n",
    "\n",
    "# Toon de klasse distributie voor SMOTE\n",
    "print('Originele dataset shape %s' % Counter(y))\n",
    "\n",
    "# Pas SMOTE toe\n",
    "sm = SMOTE(random_state=42)\n",
    "X_res, y_res = sm.fit_resample(X, y)\n",
    "\n",
    "# Toon de nieuwe klasse distributie\n",
    "print('Resampled dataset shape %s' % Counter(y_res))\n",
    "\n",
    "# Toon de originele klasse distributie\n",
    "print('Originele dataset shape %s' % Counter(y))\n",
    "\n",
    "# Configureer de RandomUnderSampler\n",
    "rus = RandomUnderSampler(random_state=42)\n",
    "X_res_under, y_res_under = rus.fit_resample(X, y)\n",
    "\n",
    "# Toon de nieuwe klasse distributie\n",
    "print('Dataset shape na undersampling %s' % Counter(y_res_under))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# en nu gaan we op basis van de target value (\"Heart risk\") groeperen en kijken wat de impact is van de diverse\n",
    "# categorische variabelen\n",
    "\n",
    "index = 0\n",
    "grouped = df.groupby(df_label_col)\n",
    "df_group_cols = list(set(df_cat_col) - set(df_label_col))\n",
    "# plot 13 grafieken, \n",
    "if plot_graphs:\n",
    "    for j in [5, 4, 4]:\n",
    "        fig, axes = plt.subplots(ncols=j, figsize=(15, 6))\n",
    "        for i in range(j):\n",
    "            grouped[df_group_cols[index]].value_counts().unstack().plot(kind=\"bar\", stacked=True, ax=axes[i])\n",
    "            bar_labels(axes[i], 0, \"center\")\n",
    "            axes[i].set_title(df_group_cols[index].replace('_', ' '))\n",
    "            index+=1\n",
    "        plt.tight_layout()\n",
    "        save_fig(f\"Categorische features per value van de target value deel {r}\")\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hier beginnen we met de modellen\n",
    "\n",
    "Eerst een eenvoudige logistic regression, met stratefy op de target value\n",
    "Eerst zonder SMOTE, dan met parameter class_weight = 'balanced\" en dan eens met SMOTE om te zien wat het effect is van het aanpakken van de unbalanced data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eenvoudige Logistic regression met pipeline opzet \n",
    "# met stratefy op de target value\n",
    "# en zonder\n",
    "\n",
    "log_info(f\"Eenvoudige logistic regression met stratify op de target value zonder SMOTE\")\n",
    "log_info(\"**************************************************************\")\n",
    "\n",
    "df = df_raw.copy()\n",
    "\n",
    "# Separate features (X) and target variable (y)\n",
    "X = df.drop(columns=['Heart_Attack_Risk'])\n",
    "y = df['Heart_Attack_Risk']\n",
    "\n",
    "# Splitting the data into training and testing sets (80% train, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define categorical and numerical column names\n",
    "df_col = X.columns  \n",
    "df_num_col = ['Age', 'BMI', 'Cholesterol_Level', 'Resting_BP', 'Heart_Rate', 'Max_Heart_Rate_Achieved']\n",
    "df_cat_col = list(set(df_col) - set(df_num_col))\n",
    "\n",
    "num_pipeline = Pipeline([   \n",
    "    (\"standardize_numerical\", MinMaxScaler(feature_range=(-1, 1)))\n",
    "    # (\"standardize_numerical\", StandardScaler())\n",
    "])\n",
    "\n",
    "cat_pipeline = Pipeline(steps=[   \n",
    "    ('encode_categorical', OneHotEncoder(drop='first'))\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer([\n",
    "    (\"numerical\", num_pipeline, df_num_col),\n",
    "    (\"categorical\", cat_pipeline, df_cat_col)],\n",
    "     remainder='passthrough')\n",
    "\n",
    "# Create the pipeline with preprocessing and logistic regression\n",
    "pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),       \n",
    "    ('regressor', LogisticRegression())\n",
    "])\n",
    "\n",
    "# Fit the pipeline on the training data\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Making predictions on the test set\n",
    "y_pred = pipeline.predict(X_test)\n",
    "\n",
    "y_pred_counter = Counter(y_pred)\n",
    "y_test_counter = Counter(y_test)\n",
    "log_info(f\"Voorkomens in y_pred: {y_pred_counter}\")\n",
    "log_info(f\"Voorkomens in y_test: {y_test_counter}\")\n",
    "\n",
    "acc_score = accuracy_score(y_pred, y_test)*100\n",
    "class_report = classification_report(y_pred, y_test)\n",
    "conf_matrix = confusion_matrix(y_pred, y_test)\n",
    "\n",
    "log_info(f\"\\naccuracy_score: {acc_score}\")\n",
    "log_info(f\"classification_report: {class_report}\")\n",
    "summary_conf_matrix = tabulate(conf_matrix, headers='keys', tablefmt='psql')\n",
    "log_info(f\"Confusion matrix: {summary_conf_matrix}\")\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred, labels=pipeline.classes_)\n",
    "\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm,\n",
    "                              display_labels=pipeline.classes_)\n",
    "disp.plot()\n",
    "save_fig(\"Confusion matrix - example logistic regression zonder SMOTE\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eenvoudige Logistic regression met pipeline opzet \n",
    "# met stratefy op de target value\n",
    "# en nu eens met de parameter class_weight='balanced' voor de logistic regression\n",
    "\n",
    "log_info(f\"Eenvoudige logistic regression met stratify op de target value en class weight = balanced\")\n",
    "log_info(\"******************************************************************************************\")\n",
    "\n",
    "df = df_raw.copy()\n",
    "\n",
    "# Separate features (X) and target variable (y)\n",
    "X = df.drop(columns=['Heart_Attack_Risk'])\n",
    "y = df['Heart_Attack_Risk']\n",
    "\n",
    "# Splitting the data into training and testing sets (80% train, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define categorical and numerical column names\n",
    "df_col = X.columns  \n",
    "df_num_col = ['Age', 'BMI', 'Cholesterol_Level', 'Resting_BP', 'Heart_Rate', 'Max_Heart_Rate_Achieved']\n",
    "df_cat_col = list(set(df_col) - set(df_num_col))\n",
    "\n",
    "num_pipeline = Pipeline([   \n",
    "    (\"standardize_numerical\", MinMaxScaler(feature_range=(-1, 1)))\n",
    "    # (\"standardize_numerical\", StandardScaler())\n",
    "])\n",
    "\n",
    "cat_pipeline = Pipeline(steps=[   \n",
    "    ('encode_categorical', OneHotEncoder(drop='first'))\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer([\n",
    "    (\"numerical\", num_pipeline, df_num_col),\n",
    "    (\"categorical\", cat_pipeline, df_cat_col)],\n",
    "     remainder='passthrough')\n",
    "\n",
    "# Create the pipeline with preprocessing and logistic regression\n",
    "pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),       \n",
    "    ('regressor', LogisticRegression(class_weight='balanced'))\n",
    "])\n",
    "\n",
    "# Fit the pipeline on the training data\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Making predictions on the test set\n",
    "y_pred = pipeline.predict(X_test)\n",
    "\n",
    "y_pred_counter = Counter(y_pred)\n",
    "y_test_counter = Counter(y_test)\n",
    "log_info(f\"Voorkomens in y_pred: {y_pred_counter}\")\n",
    "log_info(f\"Voorkomens in y_test: {y_test_counter}\")\n",
    "\n",
    "acc_score = accuracy_score(y_pred, y_test)*100\n",
    "class_report = classification_report(y_pred, y_test)\n",
    "conf_matrix = confusion_matrix(y_pred, y_test)\n",
    "\n",
    "log_info(f\"\\naccuracy_score: {acc_score}\")\n",
    "log_info(f\"classification_report: {class_report}\")\n",
    "summary_conf_matrix = tabulate(conf_matrix, headers='keys', tablefmt='psql')\n",
    "log_info(f\"Confusion matrix: {summary_conf_matrix}\")\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred, labels=pipeline.classes_)\n",
    "\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm,\n",
    "                              display_labels=pipeline.classes_)\n",
    "disp.plot()\n",
    "save_fig(\"Confusion matrix - example logistic regression met class weight = balanced\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eenvoudige logistic regression met pipeline opzet en met SMOTE\n",
    "Om te checken of SMOTE bijdraagt tot een beter resultaat\n",
    "\n",
    "Maar dat is niet. Voor de vervolg regressie passen we geen SMOTE meer toe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eenvoudige Logistic regression met pipeline opzet en met SMOTE\n",
    "# met stratefy op de target value\n",
    "\n",
    "log_info(f\"Eenvoudige logistic regression met SMOTE en met stratify op de target value\")\n",
    "log_info(\"********************************************************************\")\n",
    "\n",
    "df = df_raw.copy()\n",
    "\n",
    "# Separate features (X) and target variable (y)\n",
    "X = df.drop(columns=['Heart_Attack_Risk'])\n",
    "y = df['Heart_Attack_Risk']\n",
    "\n",
    "# Splitting the data into training and testing sets (80% train, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define categorical and numerical column names\n",
    "df_col = X.columns  \n",
    "df_num_col = ['Age', 'BMI', 'Cholesterol_Level', 'Resting_BP', 'Heart_Rate', 'Max_Heart_Rate_Achieved']\n",
    "df_cat_col = list(set(df_col) - set(df_num_col))\n",
    "\n",
    "num_pipeline = Pipeline([   \n",
    "    (\"standardize_numerical\", MinMaxScaler(feature_range=(-1, 1)))\n",
    "    # (\"standardize_numerical\", StandardScaler())\n",
    "])\n",
    "\n",
    "cat_pipeline = Pipeline(steps=[   \n",
    "    ('encode_categorical', OneHotEncoder(drop='first'))\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer([\n",
    "    (\"numerical\", num_pipeline, df_num_col),\n",
    "    (\"categorical\", cat_pipeline, df_cat_col)],\n",
    "     remainder='passthrough')\n",
    "\n",
    "# Create the pipeline with preprocessing and logistic regression\n",
    "pipeline = ImbPipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('SMOTE', SMOTE(random_state=42)),      \n",
    "    ('regressor', LogisticRegression(class_weight='balanced'))\n",
    "])\n",
    "\n",
    "# Fit the pipeline on the training data\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Making predictions on the test set\n",
    "y_pred = pipeline.predict(X_test)\n",
    "\n",
    "y_pred_counter = Counter(y_pred)\n",
    "y_test_counter = Counter(y_test)\n",
    "log_info(f\"Voorkomens in y_pred: {y_pred_counter}\")\n",
    "log_info(f\"Voorkomens in y_test: {y_test_counter}\")\n",
    "\n",
    "acc_score = accuracy_score(y_test, y_pred)*100\n",
    "class_report = classification_report(y_test, y_pred)\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "log_info(f\"\\naccuracy_score: {acc_score}\")\n",
    "log_info(f\"classification_report: {class_report}\")\n",
    "summary_conf_matrix = tabulate(conf_matrix, headers='keys', tablefmt='psql')\n",
    "log_info(f\"Confusion matrix: {summary_conf_matrix}\")\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred, labels=pipeline.classes_)\n",
    "\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm,\n",
    "                              display_labels=pipeline.classes_)\n",
    "disp.plot()\n",
    "save_fig(\"Confusion matrix - example logistic regression met SMOTE\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nu gaan we voor een aantal classificatie modellen met opzet van pipeling.\n",
    "\n",
    "Zonder SMOTE omdat dit niet echt bijdraagt tot het resultaat, wel met de parameter class_weight='balanced' voor LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# en nu gaan we voor een aantal classificatie modellen dit uitvoeren\n",
    "# en checken wat het beste model is tot nu toe\n",
    "# zonder SMOTE omdat we merken dat SMOTE niet bijdraagt aan een betere score\n",
    "# we gebruiken wel de parameter class_weight='balanced' om de onbalans in de target value te compenseren\n",
    "\n",
    "# duurt enkele minuten\n",
    "\n",
    "rfc = RandomForestClassifier()\n",
    "abc = AdaBoostClassifier()\n",
    "gbc = GradientBoostingClassifier()\n",
    "etc = ExtraTreesClassifier()\n",
    "svc = SVC()\n",
    "lgr = LogisticRegression(class_weight='balanced')\n",
    "\n",
    "models_cl = [rfc, abc, gbc, etc, lgr, svc]\n",
    "models_cl = [lgr, svc, abc]\n",
    "\n",
    "names_cl = [\"Random Forest\", \"Ada Boost\", \"Gradient Boosting\", \"Extra Trees\",\n",
    "        \"Logistic Regression\", \"Support Vector Machine\"]\n",
    "names_cl = [\"Logistic Regression\", \"Support Vector Machine\",\"Gradient Boosting\"]\n",
    "\n",
    "def training_classification(X_train, X_test, y_train, y_test, preprocessor):    \n",
    "    scores, scores_dict, reports, cms = [], dict(), dict(), dict()\n",
    "    for i, j in zip(models_cl, names_cl):\n",
    "        pipeline = Pipeline([\n",
    "            ('preprocessor', preprocessor),                  \n",
    "            ('regressor', i)            \n",
    "            ])\n",
    "        pipeline.fit(X_train, y_train)\n",
    "        y_pred = pipeline.predict(X_test)\n",
    "        acc_score = accuracy_score(y_test, y_pred)*100\n",
    "        scores += [acc_score]\n",
    "        scores_dict[j] = [acc_score]    \n",
    "        reports[j] = classification_report(y_test, y_pred)\n",
    "        cms[j] = confusion_matrix(y_test, y_pred)\n",
    "        log_info(f\"\\n\\nModel : {j}\")\n",
    "        log_info(\"\\n\")\n",
    "        log_info(f\"Accuracy scores: {acc_score}\")\n",
    "        log_info(f\"Confusion matrix: {cms[j]}\") \n",
    "        log_info(f\"Classification report: {reports[j]}\") \n",
    "        disp = ConfusionMatrixDisplay(confusion_matrix=cm,\n",
    "                              display_labels=pipeline.classes_)\n",
    "        disp.plot()\n",
    "        fig_titel = f\"ConfusionMatrixDisplay voor model {i}\"\n",
    "        save_fig(fig_titel)\n",
    "\n",
    "    fig, axes = plt.subplots()\n",
    "    df_results = pd.DataFrame({\"score\": scores}, index=names_cl)\n",
    "    df_results = df_results.sort_values(\"score\", ascending=False)\n",
    "\n",
    "    index = 0\n",
    "    for row in range(1):\n",
    "        fig, axes = plt.subplots(ncols=3, figsize=(15, 6))        \n",
    "        for i in range(3):\n",
    "            sns.heatmap(cms[df_results.index[index]], annot=True, fmt='d', ax=axes[i])\n",
    "            axes[i].set_title(\"{}: {}%\".format(df_results.index[index], df_results.iloc[index, 0]))\n",
    "            index += 1\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "   \n",
    "    return scores_dict, reports, cms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_raw.copy()\n",
    "\n",
    "# Separate features (X) and target variable (y)\n",
    "X = df.drop(columns=['Heart_Attack_Risk'])\n",
    "y = df['Heart_Attack_Risk']\n",
    "\n",
    "# Splitting the data into training and testing sets (80% train, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.2, random_state=42)\n",
    "\n",
    "scores_dict, reports_dict, cms_dict = training_classification(X_train, X_test, y_train, y_test, preprocessor)\n",
    "\n",
    "#results_string = tabulate(results, headers='keys', tablefmt='psql')\n",
    "for models in scores_dict.keys():\n",
    "    log_info(\"\\n\")\n",
    "    log_info(f\"Resultaten voor model {models}\")\n",
    "    log_info(\"**********************************************\\n\")\n",
    "    log_info(f\"Score: {scores_dict[models]}\")\n",
    "    log_info(f\"Classification report: {reports_dict[models]}\")\n",
    "    log_info(f\"Confusion matrix: {cms_dict[models]}\")\n",
    "    log_info(\"\\n\\n\")\n",
    "\n",
    "# plot de scores in dict in een bar plot ten opzichte van de modellen\n",
    "# Plot the scores in a bar plot\n",
    "model_names = list(scores_dict.keys())\n",
    "accuracies = [scores_dict[model][0] for model in model_names]\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.bar(model_names, accuracies, color='blue')\n",
    "plt.xlabel('Model')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Accuracy of Different Models')\n",
    "plt.xticks(rotation=45)\n",
    "save_fig(\"Vergelijking van modellen voor classificatie zonder Hyperparameter tuning\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En nu gaan we de hyperparameters van de diverse modellen tunen  met GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_classification_models_with_pipeline_hyperparameter_tuning(X_train, X_test, y_train, y_test, preprocessor):\n",
    "    # Hyperparameters sets voor verschillende modellen\n",
    "\n",
    "    models = {\n",
    "        'Logistic Regression': LogisticRegression(),\n",
    "        'Decision Tree': DecisionTreeClassifier(random_state=42),\n",
    "        'Gradient Boosting': GradientBoostingClassifier(random_state=42)\n",
    "    }\n",
    "\n",
    "    param_grid = {\n",
    "        'Logistic Regression': {\n",
    "            'classifier__C': [0.01, 0.1, 1.0],  # Regularisatieparameter\n",
    "            'classifier__solver': ['liblinear', 'lbfgs']  # Solvers die verschillende penalties ondersteunen\n",
    "        },\n",
    "        'Gradient Boosting': {\n",
    "            'classifier__n_estimators': [100, 200],\n",
    "            'classifier__learning_rate': [0.1, 0.05, 0.01]\n",
    "        },\n",
    "        'Decision Tree': {\n",
    "            'classifier__criterion': ['gini', 'entropy'],\n",
    "            'classifier__max_depth': [None, 10, 20, 30],\n",
    "            'classifier__min_samples_split': [2, 5, 10],\n",
    "            'classifier__min_samples_leaf': [1, 2, 4]\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    results = {}\n",
    "    for model_name, model in models.items():\n",
    "        log_info(f\"Executing execute_classification_models_with_pipeline_hyperparameter_tuning for model: {model_name}\")\n",
    "        pipeline = Pipeline([\n",
    "            ('preprocessor', preprocessor),           \n",
    "            ('classifier', model)\n",
    "        ])     \n",
    "\n",
    "        grid_search = GridSearchCV(pipeline, param_grid[model_name], scoring=\"f1_weighted\", cv=5, refit='f1')\n",
    "        grid_search.fit(X_train, y_train)\n",
    "        best_model = grid_search.best_estimator_\n",
    "        y_pred = best_model.predict(X_test)\n",
    "\n",
    "        results[model_name] = grid_search.best_estimator_\n",
    "        best_score = grid_search.best_score_\n",
    "\n",
    "        test_score = best_model.score(X_test, y_test)\n",
    "        #print(\"Score op de testset:\", test_score)\n",
    "\n",
    "        y_pred = best_model.predict(X_test)\n",
    "        conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "        class_report = classification_report(y_test, y_pred)   \n",
    "        #print(test_score)\n",
    "        #print(conf_matrix)\n",
    "        #print(class_report)\n",
    "        results[model_name] = {'test_score': test_score, 'conf_matrix': conf_matrix, 'class_report': class_report}        \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m16/02/2025 09:52:05 : Info : Executing execute_classification_models_with_pipeline_hyperparameter_tuning for model: Logistic Regression\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\data\\repos\\data_science_cases\\myenv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\data\\repos\\data_science_cases\\myenv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\data\\repos\\data_science_cases\\myenv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m16/02/2025 09:52:14 : Info : Executing execute_classification_models_with_pipeline_hyperparameter_tuning for model: Decision Tree\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 32\u001b[0m\n\u001b[0;32m     26\u001b[0m pipeline \u001b[38;5;241m=\u001b[39m Pipeline([\n\u001b[0;32m     27\u001b[0m     (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpreprocessor\u001b[39m\u001b[38;5;124m'\u001b[39m, preprocessor),       \n\u001b[0;32m     28\u001b[0m     (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mregressor\u001b[39m\u001b[38;5;124m'\u001b[39m, LogisticRegression(class_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbalanced\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[0;32m     29\u001b[0m ])\n\u001b[0;32m     31\u001b[0m X_train, X_test, y_train, y_test \u001b[38;5;241m=\u001b[39m train_test_split(X,y,test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[1;32m---> 32\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mexecute_classification_models_with_pipeline_hyperparameter_tuning\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreprocessor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     34\u001b[0m df_results \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(results, columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreport\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcms\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     35\u001b[0m resultaat_string \u001b[38;5;241m=\u001b[39m tabulate(df_results, headers\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mkeys\u001b[39m\u001b[38;5;124m'\u001b[39m, tablefmt\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpsql\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[1;32mIn[12], line 36\u001b[0m, in \u001b[0;36mexecute_classification_models_with_pipeline_hyperparameter_tuning\u001b[1;34m(X_train, X_test, y_train, y_test, preprocessor)\u001b[0m\n\u001b[0;32m     30\u001b[0m pipeline \u001b[38;5;241m=\u001b[39m Pipeline([\n\u001b[0;32m     31\u001b[0m     (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpreprocessor\u001b[39m\u001b[38;5;124m'\u001b[39m, preprocessor),           \n\u001b[0;32m     32\u001b[0m     (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclassifier\u001b[39m\u001b[38;5;124m'\u001b[39m, model)\n\u001b[0;32m     33\u001b[0m ])     \n\u001b[0;32m     35\u001b[0m grid_search \u001b[38;5;241m=\u001b[39m GridSearchCV(pipeline, param_grid[model_name], scoring\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mf1_weighted\u001b[39m\u001b[38;5;124m\"\u001b[39m, cv\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, refit\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mf1\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 36\u001b[0m \u001b[43mgrid_search\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     37\u001b[0m best_model \u001b[38;5;241m=\u001b[39m grid_search\u001b[38;5;241m.\u001b[39mbest_estimator_\n\u001b[0;32m     38\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m best_model\u001b[38;5;241m.\u001b[39mpredict(X_test)\n",
      "File \u001b[1;32mc:\\data\\repos\\data_science_cases\\myenv\\Lib\\site-packages\\sklearn\\base.py:1473\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1466\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1468\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1469\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1470\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1471\u001b[0m     )\n\u001b[0;32m   1472\u001b[0m ):\n\u001b[1;32m-> 1473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\data\\repos\\data_science_cases\\myenv\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1019\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[1;34m(self, X, y, **params)\u001b[0m\n\u001b[0;32m   1013\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_results(\n\u001b[0;32m   1014\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[0;32m   1015\u001b[0m     )\n\u001b[0;32m   1017\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[1;32m-> 1019\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevaluate_candidates\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1021\u001b[0m \u001b[38;5;66;03m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[0;32m   1022\u001b[0m \u001b[38;5;66;03m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[0;32m   1023\u001b[0m first_test_score \u001b[38;5;241m=\u001b[39m all_out[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_scores\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32mc:\\data\\repos\\data_science_cases\\myenv\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1573\u001b[0m, in \u001b[0;36mGridSearchCV._run_search\u001b[1;34m(self, evaluate_candidates)\u001b[0m\n\u001b[0;32m   1571\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_run_search\u001b[39m(\u001b[38;5;28mself\u001b[39m, evaluate_candidates):\n\u001b[0;32m   1572\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Search all candidates in param_grid\"\"\"\u001b[39;00m\n\u001b[1;32m-> 1573\u001b[0m     \u001b[43mevaluate_candidates\u001b[49m\u001b[43m(\u001b[49m\u001b[43mParameterGrid\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparam_grid\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\data\\repos\\data_science_cases\\myenv\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:965\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[1;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[0;32m    957\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    958\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[0;32m    959\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFitting \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m folds for each of \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m candidates,\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    960\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m totalling \u001b[39m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m fits\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m    961\u001b[0m             n_splits, n_candidates, n_candidates \u001b[38;5;241m*\u001b[39m n_splits\n\u001b[0;32m    962\u001b[0m         )\n\u001b[0;32m    963\u001b[0m     )\n\u001b[1;32m--> 965\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mparallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    966\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_fit_and_score\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    967\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_estimator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    968\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    969\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    970\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    971\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtest\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    972\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparameters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    973\u001b[0m \u001b[43m        \u001b[49m\u001b[43msplit_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_splits\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    974\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcandidate_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_candidates\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    975\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_and_score_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    976\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    977\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mproduct\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    978\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcandidate_params\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    979\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mrouted_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplitter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    980\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    981\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    983\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    984\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    985\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo fits were performed. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    986\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWas the CV iterator empty? \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    987\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWere there no candidates?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    988\u001b[0m     )\n",
      "File \u001b[1;32mc:\\data\\repos\\data_science_cases\\myenv\\Lib\\site-packages\\sklearn\\utils\\parallel.py:74\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     69\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[0;32m     70\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     71\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[0;32m     73\u001b[0m )\n\u001b[1;32m---> 74\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\data\\repos\\data_science_cases\\myenv\\Lib\\site-packages\\joblib\\parallel.py:1918\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1916\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_sequential_output(iterable)\n\u001b[0;32m   1917\u001b[0m     \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[1;32m-> 1918\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1920\u001b[0m \u001b[38;5;66;03m# Let's create an ID that uniquely identifies the current call. If the\u001b[39;00m\n\u001b[0;32m   1921\u001b[0m \u001b[38;5;66;03m# call is interrupted early and that the same instance is immediately\u001b[39;00m\n\u001b[0;32m   1922\u001b[0m \u001b[38;5;66;03m# re-used, this id will be used to prevent workers that were\u001b[39;00m\n\u001b[0;32m   1923\u001b[0m \u001b[38;5;66;03m# concurrently finalizing a task from the previous call to run the\u001b[39;00m\n\u001b[0;32m   1924\u001b[0m \u001b[38;5;66;03m# callback.\u001b[39;00m\n\u001b[0;32m   1925\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n",
      "File \u001b[1;32mc:\\data\\repos\\data_science_cases\\myenv\\Lib\\site-packages\\joblib\\parallel.py:1847\u001b[0m, in \u001b[0;36mParallel._get_sequential_output\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1845\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_batches \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   1846\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m-> 1847\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1848\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_completed_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   1849\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprint_progress()\n",
      "File \u001b[1;32mc:\\data\\repos\\data_science_cases\\myenv\\Lib\\site-packages\\sklearn\\utils\\parallel.py:136\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    134\u001b[0m     config \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m    135\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig):\n\u001b[1;32m--> 136\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\data\\repos\\data_science_cases\\myenv\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:888\u001b[0m, in \u001b[0;36m_fit_and_score\u001b[1;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, score_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, split_progress, candidate_progress, error_score)\u001b[0m\n\u001b[0;32m    886\u001b[0m         estimator\u001b[38;5;241m.\u001b[39mfit(X_train, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\n\u001b[0;32m    887\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 888\u001b[0m         \u001b[43mestimator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    890\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m    891\u001b[0m     \u001b[38;5;66;03m# Note fit time as time until error\u001b[39;00m\n\u001b[0;32m    892\u001b[0m     fit_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start_time\n",
      "File \u001b[1;32mc:\\data\\repos\\data_science_cases\\myenv\\Lib\\site-packages\\sklearn\\base.py:1473\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1466\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1468\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1469\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1470\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1471\u001b[0m     )\n\u001b[0;32m   1472\u001b[0m ):\n\u001b[1;32m-> 1473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\data\\repos\\data_science_cases\\myenv\\Lib\\site-packages\\sklearn\\pipeline.py:473\u001b[0m, in \u001b[0;36mPipeline.fit\u001b[1;34m(self, X, y, **params)\u001b[0m\n\u001b[0;32m    471\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_final_estimator \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpassthrough\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    472\u001b[0m         last_step_params \u001b[38;5;241m=\u001b[39m routed_params[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m][\u001b[38;5;241m0\u001b[39m]]\n\u001b[1;32m--> 473\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_final_estimator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mXt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mlast_step_params\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfit\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    475\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32mc:\\data\\repos\\data_science_cases\\myenv\\Lib\\site-packages\\sklearn\\base.py:1473\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1466\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1468\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1469\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1470\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1471\u001b[0m     )\n\u001b[0;32m   1472\u001b[0m ):\n\u001b[1;32m-> 1473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\data\\repos\\data_science_cases\\myenv\\Lib\\site-packages\\sklearn\\tree\\_classes.py:1009\u001b[0m, in \u001b[0;36mDecisionTreeClassifier.fit\u001b[1;34m(self, X, y, sample_weight, check_input)\u001b[0m\n\u001b[0;32m    978\u001b[0m \u001b[38;5;129m@_fit_context\u001b[39m(prefer_skip_nested_validation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    979\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y, sample_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, check_input\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m    980\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Build a decision tree classifier from the training set (X, y).\u001b[39;00m\n\u001b[0;32m    981\u001b[0m \n\u001b[0;32m    982\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1006\u001b[0m \u001b[38;5;124;03m        Fitted estimator.\u001b[39;00m\n\u001b[0;32m   1007\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1009\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1010\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1011\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1012\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1013\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheck_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcheck_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1014\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1015\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32mc:\\data\\repos\\data_science_cases\\myenv\\Lib\\site-packages\\sklearn\\tree\\_classes.py:472\u001b[0m, in \u001b[0;36mBaseDecisionTree._fit\u001b[1;34m(self, X, y, sample_weight, check_input, missing_values_in_feature_mask)\u001b[0m\n\u001b[0;32m    461\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    462\u001b[0m     builder \u001b[38;5;241m=\u001b[39m BestFirstTreeBuilder(\n\u001b[0;32m    463\u001b[0m         splitter,\n\u001b[0;32m    464\u001b[0m         min_samples_split,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    469\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmin_impurity_decrease,\n\u001b[0;32m    470\u001b[0m     )\n\u001b[1;32m--> 472\u001b[0m \u001b[43mbuilder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuild\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtree_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmissing_values_in_feature_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    474\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_outputs_ \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m is_classifier(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    475\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_classes_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_classes_[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# we gaan eerst terug de pipelin opzetten\n",
    "# deze kan aangepast worden als we andere features willen gebruiken\n",
    "df = df_raw.copy() \n",
    "X = df.drop('Heart_Attack_Risk', axis=1)\n",
    "y = df['Heart_Attack_Risk']\n",
    "\n",
    "df_col = X.columns  \n",
    "df_num_col = ['Age', 'BMI', 'Cholesterol_Level', 'Resting_BP', 'Heart_Rate', 'Max_Heart_Rate_Achieved']\n",
    "df_cat_col = list(set(df_col) - set(df_num_col))\n",
    "\n",
    "num_pipeline = Pipeline([   \n",
    "    (\"standardize_numerical\", MinMaxScaler(feature_range=(-1, 1)))\n",
    "    # (\"standardize_numerical\", StandardScaler())\n",
    "])\n",
    "\n",
    "cat_pipeline = Pipeline(steps=[   \n",
    "    ('encode_categorical', OneHotEncoder(drop='first'))\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer([\n",
    "    (\"numerical\", num_pipeline, df_num_col),\n",
    "    (\"categorical\", cat_pipeline, df_cat_col)],\n",
    "     remainder='passthrough')\n",
    "\n",
    "# Create the pipeline with preprocessing and logistic regression\n",
    "pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),       \n",
    "    ('regressor', LogisticRegression(class_weight='balanced'))\n",
    "])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2, random_state=42)\n",
    "results = execute_classification_models_with_pipeline_hyperparameter_tuning(X_train, X_test, y_train, y_test, preprocessor)\n",
    "\n",
    "df_results = pd.DataFrame(results, columns=['model', 'accuracy', 'report', 'cms'])\n",
    "resultaat_string = tabulate(df_results, headers='keys', tablefmt='psql')\n",
    "\n",
    "log_info(\"\\n\")\n",
    "log_info(f\"Resultaten voor model met hyperparameter tuning\")\n",
    "log_info(\"**********************************************\")\n",
    "log_info(f\"voor models : {results.keys()}\\n\")\n",
    "\n",
    "for keys, items in results.items():\n",
    "    log_info(f\"Resultaat voor model {keys}\")     \n",
    "    for k, item in items.items():\n",
    "        log_info(f\"Parameter: {k}\")     \n",
    "        log_info(f\"{item}\")    \n",
    "    log_info(\"\\n\\n\")\n",
    "\n",
    "\n",
    "# haal uit de resultsdict alle waarden van de accuracy en plot deze per model\n",
    "# en maak een confusion matrix voor elk model\n",
    "model_names = list(results.keys())\n",
    "accuracies = [results[model]['test_score'] for model in model_names]\n",
    "\n",
    "# Plot the accuracies\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.bar(model_names, accuracies, color='blue')\n",
    "plt.xlabel('Model')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Accuracy of Different Models with Hyperparameter Tuning')\n",
    "plt.xticks(rotation=45)\n",
    "save_fig(\"Model_accuracies_with_hyperparameter_tuning\")\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
