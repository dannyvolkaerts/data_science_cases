{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook: Healthcare No Shows Appointments\n",
    "\n",
    "Deze notebook werd gemaakt in het kader van opleiding Data Scientist,    \n",
    "en heeft tot doel om de opgedane kennis te leren gebruiken.   \n",
    "\n",
    "Deze case hoort thuis bij <B>Supervised Learning</B> en <b>Logistic Regression</B>     \n",
    "\n",
    "De dataset komt van Kaggle: <a href=\"https://www.kaggle.com/datasets/iamtanmayshukla/healthcare-no-shows-appointments-dataset\"><i>iamtanmayshukla/healthcare-no-shows-appointments-dataset</i></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deze notebook is opgedeeld in verschillende hoofdstukken, telkens aangeduid met een Markdown."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some initial settings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Uitgevoerd op: 2025-02-15 20:55:22\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "pd.set_option('display.width', 800)\n",
    "import matplotlib\n",
    "matplotlib.use('TkAgg')\n",
    "%matplotlib inline\n",
    "seed=42\n",
    "\n",
    "# remove future warnings\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# path settings\n",
    "from pathlib import Path\n",
    "import os\n",
    "p = Path()\n",
    "download_path = p / 'data'\n",
    "output_path = p / 'output'\n",
    "images_path = p / 'images'\n",
    "filename=\"\"\n",
    "\n",
    "# Create the output directory if it does not exist\n",
    "if not os.path.exists(output_path):\n",
    "\t\tos.makedirs(output_path)\n",
    "# Create the images directory if it does not exist, dit keer op een andere manier dan hierboven\n",
    "if not images_path.exists():\n",
    "\timages_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Functie om de output van de gemaakte plots te bewaren\n",
    "def save_fig(fig_name, tight_layout=True, fig_extension=\"png\", resolution=300):\n",
    "    path = images_path / f\"{fig_name}.{fig_extension}\"\n",
    "    if tight_layout:\n",
    "        plt.tight_layout()\n",
    "    plt.savefig(path, format=fig_extension, dpi=resolution)\n",
    "\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "from datetime import datetime\n",
    "#import datetime\n",
    "start_time = datetime.now() # Om op het einde de uitvoertijd te berekenen\n",
    "\n",
    "\n",
    "def print_exec_time(exec_info):\n",
    "    tqdm.write(f\"✅ Uitgevoerd op: {datetime.now():%Y-%m-%d %H:%M:%S}\")\n",
    "\n",
    "get_ipython().events.register(\"post_run_cell\", print_exec_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get the data from Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kaggle.api.kaggle_api_extended import KaggleApi\n",
    "import os\n",
    "import zipfile\n",
    "\n",
    "api = KaggleApi()\n",
    "api.authenticate()\n",
    "\n",
    "print(datetime.now())\n",
    "\n",
    "if 'data\\\\' in filename: \n",
    "    print(f\"Bestaat al: {filename}\")\n",
    "    print(f\"Het bestand is al gedownload en moet niet meer worden ingelezen.\\n\")\n",
    "else:\n",
    "    # download dataset\n",
    "    kaggle_dataset_name = 'iamtanmayshukla/healthcare-no-shows-appointments-dataset' # te halen van de Kaggle.com url\n",
    "    api.dataset_download_files(kaggle_dataset_name, path=download_path, unzip=True)\n",
    "\n",
    "    # Het gedownloade bestand oproepen vanuit de download map\n",
    "    files = os.listdir(download_path)\n",
    "\n",
    "    # neem de eerste file uit de folder\n",
    "    filename1 = os.path.join(download_path, files[0])\n",
    "    # Neem de recentste file op basis van de file met de nieuwste modification time\n",
    "    file_paths = [os.path.join(download_path, file) for file in files]\n",
    "    filename2 = max(file_paths, key=os.path.getmtime)\n",
    "\n",
    "    # check filename (en path)\n",
    "    print(f\"\\nHet bestand waarmme je verder werkt is: \\n 1. {filename1} of \\n 2. {filename2}\")\n",
    "\n",
    "    # Kies met welke filename je verder wilt werken\n",
    "    filename = filename1\n",
    "\n",
    "    # Lees het bestand in\n",
    "    df_raw = pd.read_csv(filename)\n",
    "    display(df_raw.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uit te voeren om met *verse* data te starten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Herstart door de dataset opnieuw te kopiëren vanuit de originele dataset: df_raw\n",
    "dataset = df_raw.copy()\n",
    "print(dataset.shape)\n",
    "print(dataset.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Profiling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ydata profiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAAK EEN PROFILER HTML DOCUMENT DAT JE HELPT BIJ DE DATA EXPLORATIE EN LAAD HET MEE OP MET JE ANTWOORDEN OP DE SYNTRA CLOUD.\n",
    "##########################\n",
    "# PROFILER HTML DOCUMENT #\n",
    "##########################\n",
    "\n",
    "from ydata_profiling import ProfileReport\n",
    "import os\n",
    "import webbrowser\n",
    "\"\"\"\n",
    "# Generate the profiling report, kies een goede titel\n",
    "profile = ProfileReport(dataset, title=\"Dataset Healthcare No Shows Appointments Report\", explorative=True) # explorative=True om ook de correlaties te zien\n",
    "\n",
    "# Create the output directory if it does not exist\n",
    "output_dir = \"output\"\n",
    "if not os.path.exists(output_dir):\n",
    "\tos.makedirs(output_dir)\n",
    "\n",
    "# Save the report as an HTML file\n",
    "profile.to_file(os.path.join(output_dir, \"HNSA_2_profiler.html\"))\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# controleer of er al een rapport is\n",
    "profiler_file = 'HNSA_2_profiler.html'\n",
    "profiler_path = os.path.abspath(os.path.join(output_path, profiler_file))\n",
    "if os.path.exists(profiler_path):\n",
    "\tprint(f\"profiler betsaat al. Openen...\")\n",
    "else:\n",
    "\t# Generate the profiling report, kies een goede titel\n",
    "\tprofile = ProfileReport(dataset, title=\"Dataset Healthcare No Shows Appointments Report\", explorative=True) # explorative=True om ook de correlaties te zien\n",
    "\n",
    "\t# Save the report as an HTML file\n",
    "\tprofile.to_file(os.path.join(output_path,profiler_file))\n",
    "\n",
    "\t# Pad naar je bestand\n",
    "\tprofiler_path = os.path.abspath(os.path.join(output_path, profiler_file))\n",
    "\tprint(f\"profiler gemaakt: {profiler_path}\")\n",
    "\n",
    "# Open het bestand in de standaardbrowser\n",
    "webbrowser.open(f\"file://{profiler_path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Elementaire checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aantal rijen printen\n",
    "print(f\"Aantal rijen: {dataset.shape[0]}\")\n",
    "print(f\"Aantal kolommen: {dataset.shape[1]}\")\n",
    "print(\"-----------------\")\n",
    "# een overzicht van de kolommen\n",
    "print(f\"Kolommen: {dataset.columns}\")\n",
    "print(\"-----------------\")\n",
    "# Check for missing values\n",
    "if dataset.isnull().values.any():\n",
    "    print(dataset.isnull().sum()) # aantal missing values per kolom\n",
    "else:\n",
    "    print(f\"Missing values: GEEN !! \")\n",
    "print(\"-----------------\")\n",
    "# Get basic statistics\n",
    "print(f\"Statistics: (voor numerische kolommen) \\n\")\n",
    "print(dataset.describe())\n",
    "print(\"-----------------\")\n",
    "# Check the data types\n",
    "print(f\"Data types :\")\n",
    "print(dataset.dtypes)\n",
    "print(\"-----------------\")\n",
    "\n",
    "# Check unique values in categorical columns\n",
    "for col in dataset.select_dtypes(include=['object']).columns:\n",
    "    print(f\"{col} unique values: {dataset[col].nunique()}\")\n",
    "\n",
    "print(\"-----------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning     \n",
    "In HNSA_2 voeg ik twee kolommen toe met de dag van de week voor scheduled en appointment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# omzetten van PatinetId naar string en verwijder .0 op het einde van de string\n",
    "dataset['PatientId'] = dataset['PatientId'].astype(str).str.replace('.0', '', regex=False)\n",
    "# omzetten van ScheduledDay en AppointmentDay naar datetime\n",
    "dataset['ScheduledDay'] = pd.to_datetime(dataset['ScheduledDay'])\n",
    "dataset['AppointmentDay'] = pd.to_datetime(dataset['AppointmentDay'])\n",
    "\n",
    "# toevoegen van extra kolommen, zoals ScheduledDayOfWeek en AppointmentDayOfWeek\n",
    "dataset['ScheduledDayOfWeek'] = (dataset['ScheduledDay']).dt.day_name()\n",
    "dataset['AppointmentDayOfWeek'] = dataset['AppointmentDay'].dt.day_name()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check uitvoering vorig codeblok\n",
    "print(dataset.dtypes)\n",
    "dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In deze notebook maak ik een extra kolom: Day of Week voor de afspraken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "#import matplotlib\n",
    "#matplotlib.use('TkAgg')  # or 'Qt5Agg'\n",
    "#%matplotlib inline\n",
    "\n",
    "# Day of Week verdeling\n",
    "#twee subplots\n",
    "plt.figure(figsize=(12, 6))\n",
    "# Sort days on x-axis\n",
    "day_order = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday']\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.countplot(data=dataset, x='AppointmentDayOfWeek', hue='Showed_up', order=day_order)\n",
    "plt.title('Appointment Showed up by Day of the Week')\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.countplot(data=dataset, x='ScheduledDayOfWeek', hue='Showed_up', order=day_order)\n",
    "plt.title('Scheduled Day Showed up by Day of the Week')\n",
    "save_fig('HNSA_2_day_of_week')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlatie matrix   \n",
    "Eerst op de originele data   \n",
    "Daarna op de versie met de Bool-kolommen omgezet naar 0/1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Originele Matrix\n",
    "\n",
    "# Selectie kolommen met numerieke waarden\n",
    "numeric_df_raw = df_raw.select_dtypes(include=[np.number])\n",
    "\n",
    "# Bereken de correlatie matrix\n",
    "corr = numeric_df_raw.corr()\n",
    "\n",
    "# Plot heatmap\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(corr, annot=True, cmap='coolwarm', fmt='.2f')\n",
    "plt.title('Correlation Heatmap')\n",
    "save_fig('HNSA_2_correlation_heatmap')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"ArithmeticError# Nieuwe Matrix met alle (numerische) kolommen\n",
    "# wetende dat de Boolean omgezet zijn naar int 1 of 0\n",
    "\n",
    "# Selectie kolommen met numerieke waarden\n",
    "numeric_data = dataset.select_dtypes(include=[np.number])\n",
    "\n",
    "# Bereken de correlatie matrix\n",
    "corr = numeric_data.corr()\n",
    "\n",
    "# Plot heatmap\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(corr, annot=True, cmap='coolwarm', fmt='.2f')\n",
    "plt.title('Correlation Heatmap')\n",
    "save_fig('HNSA_2_correlation_heatmap2')\n",
    "plt.show()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# START Modelling  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split Train & Test    \n",
    "X dit keer ALLE kolommen (behalve Target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maar eerst alle kolommen omzetten naar getallen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# omzetten van Scholarship, Hipertension, Diabetes, Alcoholism, Handcap en SMS_received naar 0 of 1\n",
    "dataset['Scholarship'] = dataset['Scholarship'].astype(bool).astype(int)\n",
    "dataset['Hipertension'] = dataset['Hipertension'].astype(bool).astype(int)\n",
    "dataset['Diabetes'] = dataset['Diabetes'].astype(bool).astype(int)\n",
    "dataset['Alcoholism'] = dataset['Alcoholism'].astype(bool).astype(int)\n",
    "dataset['Handcap'] = dataset['Handcap'].astype(bool).astype(int)\n",
    "dataset['SMS_received'] = dataset['SMS_received'].astype(bool).astype(int)\n",
    "# TARGET: omzetten van Showed_up naar 0 of 1\n",
    "dataset['Showed_up'] = dataset['Showed_up'].astype(bool).astype(int)\n",
    "\n",
    "# Gender f/M omzetten naar 0/1 met de LabelEncoder\n",
    "encoder = LabelEncoder()\n",
    "dataset['Gender_encoded'] = encoder.fit_transform(dataset['Gender'])\n",
    "\n",
    "\n",
    "\n",
    "# verwijderen van de originele kolommen\n",
    "#dataset.drop\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dagen van de week omzetten met EEN van de twee hieronder codeblokken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dagen van de week omzetten met OHE via get dummies\n",
    "days_SDW = {'ScheduledDayOfWeek': ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday']}\n",
    "dataset = pd.get_dummies(dataset, columns=days_SDW.keys())\n",
    "days_ADW = {'AppointmentDayOfWeek': ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday']}\n",
    "dataset = pd.get_dummies(dataset, columns=days_ADW.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" UNCOMMENT OM DE OHE UIT TE VOEREN ipv get_dummies\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Werken met OHE zelf ipv getdummies:\n",
    "days = {'ScheduledDayOfWeek': ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday'],\n",
    "        'AppointmentDayOfWeek': ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday']}\n",
    "enc1 = OneHotEncoder(sparse=False)\n",
    "encoded_SDW = enc1.fit_transform(dataset[['ScheduledDayOfWeek']])\n",
    "encoded_df1 = pd.DataFrame(encoded_SDW, columns=[f'SDW_{col}' for col in enc1.categories_[0]])\n",
    "enc2 = OneHotEncoder(sparse=False)\n",
    "encoded_ADW = enc1.fit_transform(dataset[['ScheduledDayOfWeek']])\n",
    "encoded_df2 = pd.DataFrame(encoded_ADW, columns=[f'ADW_{col}' for col in enc2.categories_[0]])\n",
    "\n",
    "#Voeg samen met originele dataset\n",
    "dataset = pd.concat([dataset, encoded_df1, encoded_df2], axis=1)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nog eens hetzelfde voor Neighbourhood\n",
    "# Neighbourhood omzetten met OHE via get dummies\n",
    "dataset = pd.get_dummies(dataset, columns=['Neighbourhood'], drop_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HNSA_2    \n",
    "Hierin werk ik met alle kolommen, behalve ... (ipv een lijst features, zoals in HNSA_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Prepare data for modeling\n",
    "#features = ['PatientId', 'AppointmentID', 'ScheduledDay', 'AppointmentDay', 'Age', 'Neighbourhood', 'Scholarship', 'Hipertension', 'Diabetes', 'Alcoholism', 'Handcap', 'SMS_received', 'Showed_up', 'Date.diff', 'ScheduledDayOfWeek', 'AppointmentDayOfWeek', 'Gender_encoded']\n",
    "target = 'Showed_up'\n",
    "#X = dataset[features]\n",
    "X = dataset.drop(columns=['Gender', 'PatientId', 'AppointmentID', 'ScheduledDay', 'AppointmentDay', 'Showed_up'])\n",
    "y = dataset[target]\n",
    "\n",
    "# Split data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=seed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predictive Modeling   Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "# Train logistic regression model\n",
    "model = LogisticRegression(max_iter=1000)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "report = classification_report(y_test, y_pred)\n",
    "con_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.4f}\\n\")\n",
    "print(\"Classification Report:\")\n",
    "print(report)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(con_matrix)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "scores = cross_val_score(model, X, y, cv=5)\n",
    "print(f\"Model gebruikt: {model} \\n\")\n",
    "print(f\"Cross-Validation Scores: {scores}\")\n",
    "print(f\"Mean CV Score: {scores.mean():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "# Initialize the model\n",
    "model = RandomForestClassifier(random_state=seed)\n",
    "print(f\"Model gebruikt: {model} \\n\")\n",
    "# Train the model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# More detailed evaluation\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "\n",
    "# For ROC-AUC score\n",
    "y_pred_proba = model.predict_proba(X_test)[:, 1]  # Probabilities for the positive class\n",
    "print(f\"ROC-AUC Score: {roc_auc_score(y_test, y_pred_proba):.4f}\\n\")\n",
    "\n",
    "# ROC Curve\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_pred_proba)\n",
    "print(f\"False Positive Rate: {fpr}\")\n",
    "print(f\"True Positive Rate: {tpr}\")\n",
    "print(f\"Thresholds: {thresholds}\")\n",
    "print(len(fpr), len(tpr), len(thresholds))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "scores = cross_val_score(model, X, y, cv=5)\n",
    "print(f\"Model gebruikt: {model} \\n\")\n",
    "print(f\"Cross-Validation Scores: {scores}\")\n",
    "print(f\"Mean CV Score: {scores.mean():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SMOTE   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Create an instance of SMOTE\n",
    "smote = SMOTE()\n",
    "\n",
    "# Apply SMOTE to your training data\n",
    "X_resampled, y_resampled = smote.fit_resample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Initialize and train your model\n",
    "model = RandomForestClassifier()\n",
    "model.fit(X_resampled, y_resampled)\n",
    "\n",
    "# Predict on test data\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate performance\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"ROC-AUC Score:\", roc_auc_score(y_test, model.predict_proba(X_test)[:, 1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hoe lang heeft dit script erover gedaan?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bereken de uitvoertijd, vergeet niet om de start_time te definieren in het begin van het  script\n",
    "end_time = datetime.now()\n",
    "print(f\"Gestart om: {start_time}\\n\")\n",
    "print(f\"Eindtijd: {end_time}\\n\")\n",
    "# print uitvoertijd in seconden\n",
    "print(f\"Uitvoertijd: {end_time-start_time}\\n\")\n",
    "# print THE END in grote karakters\n",
    "\n",
    "import pyfiglet\n",
    "text = \"THE  END\"\n",
    "ascii_art = pyfiglet.figlet_format(text)\n",
    "print(ascii_art)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "homl3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
