{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook: Healthcare No Shows Appointments\n",
    "\n",
    "Deze notebook werd gemaakt in het kader van opleiding Data Scientist,    \n",
    "en heeft tot doel om de opgedane kennis te leren gebruiken.   \n",
    "\n",
    "Deze case hoort thuis bij <B>Supervised Learning</B> en <b>Logistic Regression</B>     \n",
    "\n",
    "De dataset komt van Kaggle: <a href=\"https://www.kaggle.com/datasets/iamtanmayshukla/healthcare-no-shows-appointments-dataset\"><i>iamtanmayshukla/healthcare-no-shows-appointments-dataset</i></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deze notebook is opgedeeld in verschillende hoofdstukken, telkens aangeduid met een Markdown."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some initial settings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hier volgen een aantal imports om de rest van het notebook vlotter te laten werken.    \n",
    "Voer deze best uit, zodat de volgende codeblokken niet de volledige import moeten doen en dus sneller laden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "pd.set_option('display.width', 800)\n",
    "import matplotlib\n",
    "matplotlib.use('TkAgg')\n",
    "%matplotlib inline\n",
    "seed=42\n",
    "\n",
    "# remove future warnings\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# path settings\n",
    "from pathlib import Path\n",
    "import os\n",
    "p = Path()\n",
    "download_path = p / 'data'\n",
    "output_path = p / 'output'\n",
    "images_path = p / 'images'\n",
    "filename=\"\"\n",
    "\n",
    "# Create the output directory if it does not exist\n",
    "if not os.path.exists(output_path):\n",
    "\t\tos.makedirs(output_path)\n",
    "# Create the images directory if it does not exist, dit keer op een andere manier dan hierboven\n",
    "if not images_path.exists():\n",
    "\timages_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Functie om de output van de gemaakte plots te bewaren\n",
    "def save_fig(fig_name, tight_layout=True, fig_extension=\"png\", resolution=300):\n",
    "    path = images_path / f\"{fig_name}.{fig_extension}\"\n",
    "    if tight_layout:\n",
    "        plt.tight_layout()\n",
    "    plt.savefig(path, format=fig_extension, dpi=resolution)\n",
    "\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "from datetime import datetime\n",
    "#import datetime\n",
    "start_time = datetime.now() # Om op het einde de uitvoertijd te berekenen\n",
    "\n",
    "\n",
    "def print_exec_time(exec_info):\n",
    "    tqdm.write(f\"✅ Uitgevoerd op: {datetime.now():%Y-%m-%d %H:%M:%S}\")\n",
    "\n",
    "get_ipython().events.register(\"post_run_cell\", print_exec_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get the data from Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kaggle.api.kaggle_api_extended import KaggleApi\n",
    "import os\n",
    "import zipfile\n",
    "\n",
    "api = KaggleApi()\n",
    "api.authenticate()\n",
    "\n",
    "print(datetime.now())\n",
    "\n",
    "if 'data\\\\' in filename: \n",
    "    print(f\"Bestaat al: {filename}\")\n",
    "    print(f\"Het bestand is al gedownload en moet niet meer worden ingelezen.\\n\")\n",
    "else:\n",
    "    # download dataset\n",
    "    kaggle_dataset_name = 'iamtanmayshukla/healthcare-no-shows-appointments-dataset' # te halen van de Kaggle.com url\n",
    "    api.dataset_download_files(kaggle_dataset_name, path=download_path, unzip=True)\n",
    "\n",
    "    # Het gedownloade bestand oproepen vanuit de download map\n",
    "    files = os.listdir(download_path)\n",
    "\n",
    "    # neem de eerste file uit de folder\n",
    "    filename1 = os.path.join(download_path, files[0])\n",
    "    # Neem de recentste file op basis van de file met de nieuwste modification time\n",
    "    file_paths = [os.path.join(download_path, file) for file in files]\n",
    "    filename2 = max(file_paths, key=os.path.getmtime)\n",
    "\n",
    "    # check filename (en path)\n",
    "    print(f\"\\nHet bestand waarmme je verder werkt is: \\n 1. {filename1} of \\n 2. {filename2}\")\n",
    "\n",
    "    # Kies met welke filename je verder wilt werken\n",
    "    filename = filename1\n",
    "\n",
    "    # Lees het bestand in\n",
    "    df_raw = pd.read_csv(filename)\n",
    "    display(df_raw.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uit te voeren om met *verse* data te starten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# maak een nieuwe kopie van de originele dataset, zonder deze opnieuw te moeten downloaden\n",
    "dataset = df_raw.copy()\n",
    "# Snelle controle of de nieuwe dataset goed is ingelezen\n",
    "print(dataset.shape)\n",
    "print(dataset.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Profiling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Om een eerste indruk te krijgen over de dataset, maken we gebruik van een profiler.   \n",
    "In het geëxporteerde bestand /output/profiler.html krijg je hierna een mooi overzicht van alle data waaruit deze dataset is opgebouwd."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ydata profiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################\n",
    "# PROFILER HTML DOCUMENT #\n",
    "##########################\n",
    "\n",
    "from ydata_profiling import ProfileReport\n",
    "import os\n",
    "import webbrowser\n",
    "\n",
    "# controleer of er al een rapport is\n",
    "profiler_file = 'HNSA_1_profiler.html'\n",
    "profiler_path = os.path.abspath(os.path.join(output_path, profiler_file))\n",
    "if os.path.exists(profiler_path):\n",
    "\tprint(f\"profiler betsaat al. Openen...\")\n",
    "else:\n",
    "\t# Generate the profiling report, kies een goede titel\n",
    "\tprofile = ProfileReport(dataset, title=\"Dataset Healthcare No Shows Appointments Report\", explorative=True) # explorative=True om ook de correlaties te zien\n",
    "\n",
    "\n",
    "\n",
    "\t# Save the report as an HTML file\n",
    "\tprofile.to_file(os.path.join(output_path,profiler_file))\n",
    "\n",
    "\t# Pad naar je bestand\n",
    "\tprofiler_path = os.path.abspath(os.path.join(output_path, profiler_file))\n",
    "\tprint(f\"profiler gemaakt: {profiler_path}\")\n",
    "\n",
    "# Open het bestand in de standaardbrowser\n",
    "webbrowser.open(f\"file://{profiler_path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Elementaire checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aantal rijen printen\n",
    "print(f\"Aantal rijen: {dataset.shape[0]}\")\n",
    "print(f\"Aantal kolommen: {dataset.shape[1]}\")\n",
    "print(\"-----------------\")\n",
    "# een overzicht van de kolommen\n",
    "print(f\"Kolommen: {dataset.columns}\")\n",
    "print(\"-----------------\")\n",
    "# Check for missing values\n",
    "if dataset.isnull().values.any():\n",
    "    print(dataset.isnull().sum()) # aantal missing values per kolom\n",
    "else:\n",
    "    print(f\"Missing values: GEEN !! \")\n",
    "print(\"-----------------\")\n",
    "# Get basic statistics\n",
    "print(f\"Statistics: (voor numerische kolommen) \\n\")\n",
    "print(dataset.describe())\n",
    "print(\"-----------------\")\n",
    "# Check the data types\n",
    "print(f\"Data types :\")\n",
    "print(dataset.dtypes)\n",
    "print(\"-----------------\")\n",
    "\n",
    "# Check unique values in categorical columns\n",
    "for col in dataset.select_dtypes(include=['object']).columns:\n",
    "    print(f\"{col} unique values: {dataset[col].nunique()}\")\n",
    "\n",
    "print(\"-----------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vooraleer verder te gaan , kan je best het Profiler rapport bekijken.    \n",
    "Hierin vind je oa:   \n",
    " - overzicht van de dataset\n",
    " - Alerts, met vermelding van correlations, imbalances, ...\n",
    " - Detail van alle features, , incl statistics, histogram, unieke waarden, missing, ....\n",
    " - Interactions\n",
    " - Correlations\n",
    " - Missing values\n",
    " - Sample\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Opmerking:   \n",
    "Er zijn enkele waarden ongebalanceerd en sommige zijn toch wel hoog gecorreleerd."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eerst eens de numerische kolommen bekijken"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Voor cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_num_cols(df):\n",
    "    # Selecteer numerieke kolommen\n",
    "    numeric_cols = dataset.select_dtypes(include=['int64', 'float64']).columns\n",
    "\n",
    "    # Bereken het aantal subplots dat we nodig hebben\n",
    "    n_plots = len(numeric_cols)\n",
    "\n",
    "    # Bepaal aantal rijen en kolommen voor de subplots\n",
    "    ncols = 3  # Aantal kolommen in de matrix\n",
    "    nrows = (n_plots // ncols) + (n_plots % ncols > 0)  # Het aantal rijen is afhankelijk van het aantal plots\n",
    "\n",
    "    # Maak de subplots\n",
    "    fig, axes = plt.subplots(nrows, ncols, figsize=(15, 5 * nrows))\n",
    "\n",
    "    # Flatten de assen om makkelijk te itereren\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    # Genereer een histogram voor elke numerieke kolom\n",
    "    for i, col in enumerate(numeric_cols):\n",
    "        sns.histplot(dataset[col], ax=axes[i], bins=30, kde=True)\n",
    "        axes[i].set_title(f\"Verdeling van {col}\")\n",
    "        axes[i].set_xlabel(col)\n",
    "        axes[i].set_ylabel(\"Frequentie\")\n",
    "\n",
    "    # Verberg lege subplots (indien nodig)\n",
    "    for i in range(n_plots, len(axes)):\n",
    "        axes[i].axis('off')\n",
    "\n",
    "    # Vertoon de plot\n",
    "    plt.tight_layout()\n",
    "    save_fig(\"HNSA_1_Histograms_numeric_columns\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_num_cols(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Er zijn geen missing values, duplicates, ...    \n",
    "Hierdoor moeten we geen rijen schrappen of waarden opvullen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# omzetten van PatinetId naar string en verwijder .0 op het einde van de string, dit is geen numerische waarde met betekenis\n",
    "dataset['PatientId'] = dataset['PatientId'].astype(str).str.replace('.0', '', regex=False)\n",
    "\n",
    "# omzetten van AppointmentID naar string en verwijder .0 op het einde van de string, dit is geen numerische waarde met betekenis\n",
    "dataset['AppointmentID'] = dataset['AppointmentID'].astype(str).str.replace('.0', '', regex=False)\n",
    "\n",
    "# omzetten van ScheduledDay en AppointmentDay naar datetime\n",
    "dataset['ScheduledDay'] = pd.to_datetime(dataset['ScheduledDay'])\n",
    "dataset['AppointmentDay'] = pd.to_datetime(dataset['AppointmentDay'])\n",
    "\n",
    "# Aanmaken van extra kolommen voor de dag van de week, dit zou een invloed kunnen hebben.\n",
    "dataset['AppointmentDayOfWeek'] = dataset['AppointmentDay'].dt.day_name()\n",
    "dataset['ScheduledDayOfWeek'] = dataset['ScheduledDay'].dt.day_name()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NA cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_num_cols(dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check uitvoering vorig codeblok\n",
    "print(dataset.dtypes)\n",
    "dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Opmerking:    \n",
    "Er blijven nog kolommen over als 'opject'.    \n",
    "Deze worden later nog omgezet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "#import matplotlib\n",
    "#matplotlib.use('TkAgg')  # or 'Qt5Agg'\n",
    "#%matplotlib inline\n",
    "\n",
    "# Day of Week verdeling\n",
    "#twee subplots\n",
    "plt.figure(figsize=(12, 6))\n",
    "# Sort days on x-axis\n",
    "day_order = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday']\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.countplot(data=dataset, x='AppointmentDayOfWeek', hue='Showed_up', order=day_order)\n",
    "plt.title('Appointment Showed up by Day of the Week')\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.countplot(data=dataset, x='ScheduledDayOfWeek', hue='Showed_up', order=day_order)\n",
    "plt.title('Scheduled Day Showed up by Day of the Week')\n",
    "# Afdrukken van de twee plots naast elkaar\n",
    "save_fig(\"HNSA_1_DayOfWeekShowedUp\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Age distribution\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(data=dataset, x='Age', bins=30, hue='Showed_up', multiple='stack')\n",
    "plt.title('Age Distribution by Show-up Status')\n",
    "save_fig(\"HNSA_1_Age_Distribution_by_Show_up_Status\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlatie matrix   \n",
    "Eerst op de originele data   \n",
    "Daarna op de versie met de Bool-kolommen omgezet naar 0/1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Originele Matrix\n",
    "import matplotlib\n",
    "#matplotlib.use('TkAgg')  # or 'Qt5Agg'\n",
    "#%matplotlib inline\n",
    "# Selectie kolommen met numerieke waarden\n",
    "numeric_df_raw = df_raw.select_dtypes(include=[np.number])\n",
    "\n",
    "# Bereken de correlatie matrix\n",
    "corr = numeric_df_raw.corr()\n",
    "\n",
    "# Plot heatmap\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(corr, annot=True, cmap='coolwarm', fmt='.2f')\n",
    "plt.title('Correlation Heatmap')\n",
    "save_fig(\"HNSA_1_Correlation_Heatmap\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NIET MEER VAN TOEPASSING, maar kan misschien nog van pas komen\n",
    "\n",
    "\"\"\"\n",
    "# Nieuwe Matrix met alle (numerische) kolommen\n",
    "# wetende dat de Boolean omgezet zijn naar int 1 of 0\n",
    "\n",
    "# Selectie kolommen met numerieke waarden\n",
    "numeric_data = dataset.select_dtypes(include=[np.number])\n",
    "\n",
    "# Bereken de correlatie matrix\n",
    "corr = numeric_data.corr()\n",
    "\n",
    "# Plot heatmap\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(corr, annot=True, cmap='coolwarm', fmt='.2f')\n",
    "plt.title('Correlation Heatmap')\n",
    "save_fig(\"HNSA_1_Correlation_Heatmap_All\")\n",
    "plt.show()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# START Modelling  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split Train & Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We starten met de bruikbare kolommen.    \n",
    "Later gaan we de data bewerken en opnieuw enkele modellen toepassen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Prepare data for modeling\n",
    "features = ['Age', 'Scholarship', 'Hipertension', 'Diabetes', 'Alcoholism', 'Handcap', 'SMS_received', 'Date.diff']\n",
    "target = 'Showed_up'\n",
    "X = dataset[features]\n",
    "y = dataset[target]\n",
    "\n",
    "# Split data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=seed)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predictive Modeling   Logistic Regression    \n",
    "Baseline model (eenvoudig en interpreteerbaar) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "# Train logistic regression model\n",
    "model = LogisticRegression(max_iter=1000)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "report = classification_report(y_test, y_pred)\n",
    "con_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.4f}\\n\")\n",
    "print(\"Classification Report:\")\n",
    "print(report)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(con_matrix)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "scores = cross_val_score(model, X, y, cv=5)\n",
    "print(f\"Model gebruikt: {model} \\n\")\n",
    "print(f\"Cross-Validation Scores: {scores}\")\n",
    "print(f\"Mean CV Score: {scores.mean():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "# Initialize the model\n",
    "model = RandomForestClassifier(random_state=seed)\n",
    "print(f\"Model gebruikt: {model} \\n\")\n",
    "# Train the model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# More detailed evaluation\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "\n",
    "# For ROC-AUC score\n",
    "y_pred_proba = model.predict_proba(X_test)[:, 1]  # Probabilities for the positive class\n",
    "print(f\"ROC-AUC Score: {roc_auc_score(y_test, y_pred_proba):.4f}\\n\")\n",
    "\n",
    "# ROC Curve\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_pred_proba)\n",
    "print(f\"False Positive Rate: {fpr}\")\n",
    "print(f\"True Positive Rate: {tpr}\")\n",
    "print(f\"Thresholds: {thresholds}\")\n",
    "print(len(fpr), len(tpr), len(thresholds))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "scores = cross_val_score(model, X, y, cv=5)\n",
    "print(f\"Model gebruikt: {model} \\n\")\n",
    "print(f\"Cross-Validation Scores: {scores}\")\n",
    "print(f\"Mean CV Score: {scores.mean():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We zien niet direct een verbetering bij dit model (RandomForestClassifier).     \n",
    "We gaan nu de meest gebruikte modellen vergelijken, zijnde:    \n",
    "- Logistic Regression\n",
    "- DecisionTreeClassifier\n",
    "- RandomForrestClassifier\n",
    "- XGBClassifier\n",
    "- SVC\n",
    "- KNeighborsClassifier\n",
    "- MLPClassifier\n",
    "- AdaBoostClassifier (boosting-techniek, werkt goed bij kleinere datasets)\n",
    "- GradientBoostingClassifier (populair voor gestructureerde data, zoals in Kaggle-wedstrijden)\n",
    "- ExtraTreesClassifier (variant van RandomForest met extra randomisatie)\n",
    "- LGBMClassifier (Lichtgewicht en snel, ideaal voor grote datasets)\n",
    "- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hier gaat het gebeuren, meerdere modellen vergelijken\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier, ExtraTreesClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "# Functie om modellen te trainen en evalueren\n",
    "def evaluate_models(X, y):\n",
    "    # Train-test split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=seed)\n",
    "    \n",
    "    # Definieer modellen\n",
    "    models = {\n",
    "        \"Logistic Regression\": LogisticRegression(),\n",
    "        \"Decision Tree\": DecisionTreeClassifier(),\n",
    "        \"Random Forest\": RandomForestClassifier(),\n",
    "        \"Extra Trees\": ExtraTreesClassifier(),\n",
    "        \"AdaBoost\": AdaBoostClassifier(),\n",
    "        \"Gradient Boosting\": GradientBoostingClassifier(),\n",
    "        \"XGBoost\": XGBClassifier(eval_metric='logloss'),\n",
    "        \"LGBM\": LGBMClassifier(),\n",
    "#        \"SVM\": SVC(probability=True),  # Dit duurde veel te lang om uit te voeren, deze is niet geschikt voor grotere datsets, daarom de volgende LinearSVC\n",
    "#        \"Linear SVM\": LinearSVC(),\n",
    "        \"KNN\": KNeighborsClassifier(n_neighbors=5),\n",
    "        \"Neural Network\": MLPClassifier(max_iter=500),\n",
    "    }\n",
    "\n",
    "    # Resultaten opslaan\n",
    "    scores = {}\n",
    "\n",
    "    # Train en evalueer elk model\n",
    "    for name, model in models.items():\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "        acc = accuracy_score(y_test, y_pred)\n",
    "        scores[name] = acc\n",
    "        print(f\"{name}: {acc:.4f}\")  # Print de score\n",
    "\n",
    "    # Plot de scores\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.barplot(x=list(scores.keys()), y=list(scores.values()), palette=\"viridis\")\n",
    "    plt.xlabel(\"Model\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.title(\"Vergelijking van Model Prestaties\")\n",
    "    plt.xticks(rotation=30)\n",
    "    plt.ylim(0, 1)\n",
    "    save_fig(\"Model_Prestaties\")\n",
    "    plt.show()\n",
    "\n",
    "# Roep de functie aan met de dataset X en y\n",
    "evaluate_models(X, y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Het grotere werk   \n",
    "We gaan de kolommen 'dagen' en 'neighbourhood' encoden    \n",
    "Kolom 'Age' scalen    \n",
    "Dit keer (bijna) ALLE kolommen (behalve Target) hernemen in X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "# Deze optie laat ik vallen en kies voor OHE, welke hieronder wordt uitgevoerd. \n",
    "\n",
    "# Dagen van de week omzetten met OHE via get dummies\n",
    "days_SDW = {'ScheduledDayOfWeek': ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday']}\n",
    "dataset = pd.get_dummies(dataset, columns=days_SDW.keys())\n",
    "days_ADW = {'AppointmentDayOfWeek': ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday']}\n",
    "dataset = pd.get_dummies(dataset, columns=days_ADW.keys())\n",
    "\"\"\" \n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Werken met OHE zelf ipv getdummies:\n",
    "days = {'ScheduledDayOfWeek': ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday'],\n",
    "        'AppointmentDayOfWeek': ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday']}\n",
    "\n",
    "enc1 = OneHotEncoder(sparse=False)\n",
    "encoded_SDW = enc1.fit_transform(dataset[['ScheduledDayOfWeek']])\n",
    "#encoded_df1 = pd.DataFrame(encoded_SDW, columns=[f'SDW_{col}' for col in enc1.categories_[0]])\n",
    "encoded_df1 = pd.DataFrame(encoded_SDW, columns=enc1.get_feature_names_out(['ScheduledDayOfWeek']))\n",
    "dataset = dataset.drop(columns=['ScheduledDayOfWeek'])\n",
    "enc2 = OneHotEncoder(sparse=False)\n",
    "encoded_ADW = enc1.fit_transform(dataset[['AppointmentDayOfWeek']])\n",
    "encoded_df2 = pd.DataFrame(encoded_ADW, columns=enc1.get_feature_names_out(['AppointmentDayOfWeek']))\n",
    "dataset = dataset.drop(columns=['AppointmentDayOfWeek'])\n",
    "#Voeg samen met originele dataset\n",
    "dataset = pd.concat([dataset, encoded_df1, encoded_df2], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nu gaan we de Boolean kolommen omzetten naar 0 of 1\n",
    "\n",
    "# omzetten van Scholarship, Hipertension, Diabetes, Alcoholism, Handcap en SMS_received naar 0 of 1\n",
    "dataset['Scholarship'] = dataset['Scholarship'].astype(bool).astype(int)\n",
    "dataset['Hipertension'] = dataset['Hipertension'].astype(bool).astype(int)\n",
    "dataset['Diabetes'] = dataset['Diabetes'].astype(bool).astype(int)\n",
    "dataset['Alcoholism'] = dataset['Alcoholism'].astype(bool).astype(int)\n",
    "dataset['Handcap'] = dataset['Handcap'].astype(bool).astype(int)\n",
    "dataset['SMS_received'] = dataset['SMS_received'].astype(bool).astype(int)\n",
    "# TARGET: omzetten van Showed_up naar 0 of 1\n",
    "dataset['Showed_up'] = dataset['Showed_up'].astype(bool).astype(int)\n",
    "\n",
    "# Calculate new datediff column ## Niet nodig want de originele kolom is correct :-) \n",
    "# dataset['DateDiff2'] = (dataset['AppointmentDay'] - dataset['ScheduledDay']).dt.days\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hier gaan de kolom 'Neighbourhood' ook encoderen ! \n",
    "# dit maakt van de ene kolom opeens 81 kolommen, dit is niet altijd de beste oplossing\n",
    "# Neighbourhood omzetten met OHE via get dummies\n",
    "dataset = pd.get_dummies(dataset, columns=['Neighbourhood'], drop_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "dataset['Age'] = scaler.fit_transform(dataset[['Age']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nieuwe X en y\n",
    "# maar enkelen kolommen droppen die niet nodig zijn\n",
    "\n",
    "X = dataset.drop(columns=['Gender', 'PatientId', 'AppointmentID', 'ScheduledDay', 'AppointmentDay', 'Showed_up'])\n",
    "y = dataset['Showed_up']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hier gaat het gebeuren, meerdere modellen vergelijken\n",
    "# en dit jaar gaan we voor de volledige dataset met 100+ kolommen\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier, ExtraTreesClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "# Functie om modellen te trainen en evalueren\n",
    "def evaluate_models(X, y):\n",
    "    # Schaal de features met StandardScaler\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)  # Om zeker te zijn dat alle data 'scaled' is\n",
    "\n",
    "    # Train-test split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # Definieer modellen\n",
    "    models = {\n",
    "        \"Logistic Regression\": LogisticRegression(),\n",
    "        \"Decision Tree\": DecisionTreeClassifier(),\n",
    "        \"Random Forest\": RandomForestClassifier(),\n",
    "        \"Extra Trees\": ExtraTreesClassifier(),\n",
    "        \"AdaBoost\": AdaBoostClassifier(),\n",
    "        \"Gradient Boosting\": GradientBoostingClassifier(),\n",
    "        \"XGBoost\": XGBClassifier(eval_metric='logloss'),\n",
    "        \"LGBM\": LGBMClassifier(),\n",
    "#        \"SVM\": SVC(probability=True),  # Dit duurde veel te lang om uit te voeren, deze is niet geschikt voor grotere datsets, daarom de volgende LinearSVC\n",
    "#        \"Linear SVM\": LinearSVC(max_iter=10000), # zelfs 10000 iterations failed to converge, daarom de eruit gehaald\n",
    "        \"KNN\": KNeighborsClassifier(n_neighbors=5),\n",
    "        \"Neural Network\": MLPClassifier(max_iter=500), # Deze duurt ookiets te lang om goed te zijn\n",
    "    }\n",
    "\n",
    "    # Resultaten opslaan\n",
    "    scores = {}\n",
    "\n",
    "    # Train en evalueer elk model\n",
    "    for name, model in models.items():\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "\n",
    "        # Bereken de score\n",
    "        acc = accuracy_score(y_test, y_pred)\n",
    "        scores[name] = acc\n",
    "        print(f\"{name}: {acc:.4f}\")  # Print de score\n",
    "\n",
    "    # Plot de scores\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    ax = sns.barplot(x=list(scores.keys()), y=list(scores.values()), palette=\"viridis\")\n",
    "        # Voeg de waarden boven de bars toe\n",
    "    for p in ax.patches:\n",
    "        ax.annotate(f'{p.get_height():.4f}',  # Waarde boven de bar\n",
    "                    (p.get_x() + p.get_width() / 2., p.get_height()),  # Positie van de annotatie\n",
    "                    ha='center', va='center',  # Horizontaal en verticaal uitlijnen\n",
    "                    fontsize=12, color='black',  # Stijl van de tekst\n",
    "                    xytext=(0, 5),  # Plaatsing van de tekst boven de bar\n",
    "                    textcoords='offset points')\n",
    "    # Titels en labels toevoegen\n",
    "    plt.xlabel(\"Model\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.title(\"Vergelijking van Model Prestaties\")\n",
    "    plt.xticks(rotation=30, ha='right')  # Draai de x-as labels\n",
    "    plt.ylim(0, 1) # Zet de limieten van de y-as\n",
    "    plt.tight_layout()  # Zorg ervoor dat alles netjes past\n",
    "    save_fig(\"Model_Prestaties_Full\")\n",
    "    plt.show()\n",
    "\n",
    "# Start the engines !!\n",
    "evaluate_models(X, y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SMOTE    \n",
    "Het gebruik van SMOTE (Synthetic Minority Over-sampling Technique) zou hier zeker interessant kunnen zijn, omdat we wel te maken met een imbalance dataset, waar de targetkolom een sterke klasse-imbalance vertoont (bijvoorbeeld als 90% van de waarden 'False' zijn en maar 10% 'True'), kan SMOTE een oplossing zijn om deze onbalans te verhelpen.   \n",
    "Hier spreken nvan 80/20, dus toch de moeite om eens uit te proberen.\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Split de data in train en test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=seed)\n",
    "\n",
    "# Schaal de features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Pas SMOTE toe om de balans te herstellen\n",
    "smote = SMOTE(random_state=seed)\n",
    "X_train_res, y_train_res = smote.fit_resample(X_train_scaled, y_train)\n",
    "\n",
    "# Train je model met de resampled data\n",
    "lr = LogisticRegression()\n",
    "lr.fit(X_train_res, y_train_res)\n",
    "\n",
    "# Evalueer je model\n",
    "y_pred_smote_lr = lr.predict(X_test_scaled)\n",
    "accuracy = accuracy_score(y_test, y_pred_smote_lr)\n",
    "print(f\"Model accuracy after applying SMOTE: {accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Initialize and train your model\n",
    "rfc = RandomForestClassifier()\n",
    "rfc.fit(X_train_res, y_train_res)\n",
    "\n",
    "# Predict on test data\n",
    "y_pred_smote_rfc = rfc.predict(X_test)\n",
    "\n",
    "# Evaluate performance\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred_smote_rfc)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "print(classification_report(y_test, y_pred_smote_rfc))\n",
    "print(\"ROC-AUC Score:\", roc_auc_score(y_test, rfc.predict_proba(X_test)[:, 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "# Maak en train het XGBoost-model\n",
    "model_xgb = xgb.XGBClassifier(eval_metric='logloss')\n",
    "model_xgb.fit(X_train_res, y_train_res)\n",
    "\n",
    "# Voorspel de uitkomsten op de testset\n",
    "y_pred = model_xgb.predict(X_test_scaled)\n",
    "\n",
    "# Evalueer de nauwkeurigheid\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Model accuracy after applying SMOTE: {accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GridSearchCV voor Random Forest    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# voor de hieronder uit te voeren code, ga ik verder zonder de OHE van de kolom 'Neighbourhood'\n",
    "features = ['Age', 'Scholarship', 'Hipertension', 'Diabetes', 'Alcoholism', 'Handcap', 'SMS_received', 'Date.diff', \n",
    "            'ScheduledDayOfWeek_Monday', 'ScheduledDayOfWeek_Tuesday', 'ScheduledDayOfWeek_Wednesday', 'ScheduledDayOfWeek_Thursday', \n",
    "            'ScheduledDayOfWeek_Friday', 'ScheduledDayOfWeek_Saturday', 'AppointmentDayOfWeek_Monday', 'AppointmentDayOfWeek_Tuesday', \n",
    "            'AppointmentDayOfWeek_Wednesday', 'AppointmentDayOfWeek_Thursday', 'AppointmentDayOfWeek_Friday', 'AppointmentDayOfWeek_Saturday'\n",
    "            ]\n",
    "target = 'Showed_up'\n",
    "X = dataset[features]\n",
    "y = dataset[target]\n",
    "\n",
    "# Split de data in train en test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=seed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Definieer je model\n",
    "model = RandomForestClassifier(random_state=seed)\n",
    "\n",
    "# Definieer het grid van hyperparameters\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 150],    # Aantal bomen in het bos\n",
    "    'max_depth': [5, 10],             # Diepte van de bomen  # Geen 'None' om te vermijden dat bomen te groot worden\n",
    "    'min_samples_split': [2, 5, 10],   # Minimale aantal samples om een split te maken\n",
    "    'min_samples_leaf': [1, 2, 4],     # Minimale aantal samples in een blad\n",
    "    'max_features': ['sqrt', 'log2'],  # Maximale aantal features om te splitsen # 'auto' vermijden\n",
    "}\n",
    "\n",
    "# GridSearchCV instellen\n",
    "grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=3, n_jobs=-1, verbose=2, scoring='accuracy')\n",
    "\n",
    "# Voer GridSearch uit\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Beste hyperparameters weergeven\n",
    "print(f\"Beste hyperparameters: {grid_search.best_params_}\")\n",
    "\n",
    "# Het beste model verkrijgen\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Model evalueren\n",
    "y_pred = best_model.predict(X_test)\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "print(f\"Model accuracy: {acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nieuwe poging rfc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Definieer je model\n",
    "rfc_model = RandomForestClassifier(class_weight='balanced', criterion='gini', random_state=seed)\n",
    "\n",
    "# Definieer het grid van hyperparameters\n",
    "param_grid_rfc = {\n",
    "    'n_estimators': [50, 100, 150],    # Aantal bomen in het bos\n",
    "    'max_depth': [2, 3, 6],             # Diepte van de bomen  # Geen 'None' om te vermijden dat bomen te groot worden\n",
    "    'min_samples_split': [2, 10],      # Minimale aantal samples om een split te maken\n",
    "    'min_samples_leaf': [1, 2],     # Minimale aantal samples in een blad\n",
    "    'max_features': ['sqrt'],  # Maximale aantal features om te splitsen # 'auto' vermijden # 'log2' ook weg gelaten\n",
    "}\n",
    "\n",
    "# GridSearchCV instellen\n",
    "gs_rfc = GridSearchCV(estimator=rfc_model, param_grid=param_grid_rfc, cv=3, n_jobs=-1, verbose=2, scoring='accuracy')\n",
    "\n",
    "# Voer GridSearch uit\n",
    "gs_rfc.fit(X_train, y_train)\n",
    "\n",
    "# Beste hyperparameters weergeven\n",
    "best_params_rfc = gs_rfc.best_params_\n",
    "\n",
    "# Het beste model verkrijgen\n",
    "best_model = gs_rfc.best_estimator_\n",
    "\n",
    "# Model evalueren\n",
    "y_pred = best_model.predict(X_test)\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Beste hyperparameters: {best_params_rfc}\")\n",
    "print(f\"Model accuracy: {acc:.4f}\")\n",
    "\n",
    "\n",
    "# Resultaten van alle combinaties weergeven\n",
    "results_df_rfc = pd.DataFrame(gs_rfc.cv_results_)\n",
    "results_df_rfc.to_csv(output_path / 'HNSA_1_grid_search_RFC_results.csv', index=False)\n",
    "\n",
    "# Laat een overzicht zien van de top 5 van de resultaten\n",
    "print(\"\\nTop 5 combinaties van hyperparameters:\")\n",
    "print(results_df_rfc[['param_n_estimators', 'param_max_depth', 'param_min_samples_split', \n",
    "                      'param_min_samples_leaf', 'param_max_features', 'mean_test_score', 'std_test_score']].sort_values(by='mean_test_score', ascending=False).head())\n",
    "\n",
    "# Visualiseer de prestaties van de verschillende combinaties als een heatmap\n",
    "pivot_table_rfc = results_df_rfc.pivot_table(values='mean_test_score', \n",
    "                                     index='param_max_depth', \n",
    "                                     columns='param_n_estimators')\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.heatmap(pivot_table_rfc, annot=True, cmap='viridis', fmt='.4f', linewidths=0.5)\n",
    "plt.title(\"Grid Search Resultaten: Mean Test Score per Max Depth & N Estimators\")\n",
    "plt.xlabel('Aantal Estimators')\n",
    "plt.ylabel('Max Diepte')\n",
    "save_fig(\"HNSA_1_Grid_Search_Heatmap_RandomForrest\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "nieuwe poging xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Dit is een classificatie problem en we kiezen hier voor de XG Boost Classifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# Waarschuwingen onderdrukken\n",
    "warnings.simplefilter(\"ignore\", category=UserWarning)\n",
    "\n",
    "# Definitie van het model\n",
    "xgb_model = XGBClassifier(eval_metric=\"logloss\", random_state = seed)\n",
    "\n",
    "# Definieer het grid van hyperparameters\n",
    "param_grid_xgb = {\n",
    "    'n_estimators': [100, 200, 500],     # Aantal bomen in het bos\n",
    "    'max_depth': [3, 6, 9],              # Diepte van de bomen  # Geen 'None' om te vermijden dat bomen te groot worden\n",
    "    \"learning_rate\" : [0.01, 0.1, 0.2]   # Snelheid van leren. Verlaagt de bijdrage van elke boom., meestal tss 0.01 en 0.2, kleine waarde vereisen meer bomen\n",
    "}\n",
    "\n",
    "# make a GridSearchCV object\n",
    "gs_xgb = GridSearchCV(estimator = xgb_model,\n",
    "                  param_grid = param_grid_xgb,\n",
    "                  scoring = 'accuracy', #sklearn.metrics.SCORERS.keys() to score\n",
    "                  cv = 3, # cross validation\n",
    "                  n_jobs = 1,\n",
    "                  verbose = 9 # hoeveel info wil je zien\n",
    "                  )\n",
    "# Voer GridSearch uit\n",
    "gs_xgb.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "# Beste parameters en score\n",
    "best_params = gs_xgb.best_params_\n",
    "best_score = gs_xgb.best_score_\n",
    "print(f\"Beste parameters: {best_params}\")\n",
    "print(f\"Beste model accuracy: {best_score:.4f}\")\n",
    "\n",
    "# Resultaten verzamelen in een DataFrame\n",
    "results_df_xgb = pd.DataFrame(gs_xgb.cv_results_)\n",
    "results_df_xgb.to_csv(output_path / 'grid_search_XGB_results.csv', index=False)\n",
    "\n",
    "# Maak een pivot table voor de heatmap\n",
    "pivot_table_xgb = results_df_xgb.pivot_table(values='mean_test_score', \n",
    "                                     index='param_max_depth', \n",
    "                                     columns='param_n_estimators')\n",
    "\n",
    "# Heatmap plotten\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.heatmap(pivot_table_xgb, annot=True, cmap='viridis', fmt='.4f', linewidths=0.5)\n",
    "plt.title(\"Grid Search Resultaten: Mean Test Score voor XGBoost\")\n",
    "plt.xlabel('Aantal Estimators')\n",
    "plt.ylabel('Max Diepte')\n",
    "save_fig(\"HNSA_1_Grid_Search_Heatmap_XGBoost\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bereken de uitvoertijd, vergeet niet om de start_time te definieren in het begin van het  script\n",
    "end_time = datetime.now()\n",
    "#start_time = datetime.datetime.now()\n",
    "print(f\"Gestart om: {start_time}\\n\")\n",
    "print(f\"Eindtijd: {end_time}\\n\")\n",
    "# print uitvoertijd in seconden\n",
    "print(f\"Uitvoertijd: {end_time-start_time}\\n\")\n",
    "# print THE END in grote karakters\n",
    "\n",
    "import pyfiglet\n",
    "text = \"THE  END\"\n",
    "ascii_art = pyfiglet.figlet_format(text)\n",
    "print(ascii_art)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "homl3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
